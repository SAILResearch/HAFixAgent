{
  "Compress_1": {
    "description": "CPIO reports unexpected EOF\nWhen unpacking an CPIO archive (made with the compress classes or even made with OSX cpio comandline tool) an EOF exception is thrown.\nHere is the testcode:\n\n        final File input = getFile(\"cmdcreated.cpio\");\n\n        final InputStream in = new FileInputStream(input);\n        CpioArchiveInputStream cin = new CpioArchiveInputStream(in);\n\n        CpioArchiveEntry entry = null;\n\n        while ((entry = (CpioArchiveEntry) cin.getNextCPIOEntry()) != null) {\n            File target = new File(dir, entry.getName());\n            final OutputStream out = new FileOutputStream(target);\n            IOUtils.copy(in, out);\n            out.close();\n        }\n\n        cin.close();\n\nStacktrace is here:\n\njava.io.EOFException\n\tat org.apache.commons.compress.archivers.cpio.CpioArchiveInputStream.readFully(CpioArchiveInputStream.java:293)\n\tat org.apache.commons.compress.archivers.cpio.CpioArchiveInputStream.getNextCPIOEntry(CpioArchiveInputStream.java:168)\n\tat org.apache.commons.compress.archivers.cpio.CpioArchiveInputStreamTest.testCpioUnpack(CpioArchiveInputStreamTest.java:26)\n\t...\n\nThis happens with the first read access to the archive. It occured while my try to improve the testcases.\n\n",
    "desc_source": "jira"
  },
  "Compress_2": {
    "description": "Ar doesn't delete correct\nWhen working on the Testcases i figured out that a deletion from an Ar Archive is not as successful as it look at first glance.\nFor example: my bla.ar file contains test1.xml and test2.xml. I delete test2.xml\n\nThe \"getNextEntry\" Method just delivers test1.xml. Looks correct.\n\nBut checking the result file at commandline brings the following:\n\n$> ar -t /tmp/dir26673/bla.ar\ntest1.xml\ntest2.xml\n\nvi shows me that there is still the test2.xml entry in the archive,\neven when getNextEntry returns null.\n\nDeleting test2.xml and adding test.txt afterward brings the following:\n\n$> ar -t /tmp/dir24825/bla.ar\ntest.txt\nar: /tmp/dir24825/bla.ar: Inappropriate file type or format",
    "desc_source": "jira"
  },
  "Compress_3": {
    "description": "Are the public finish() methods ArchiveOutputStream implementations necessary and safe?\nSome of the ArchiveOutputStream implementations have public finish() methods. These are currently only called from the close() methods.\n\nSeems to me that there is no need to allow the finish() methods to be called externally, and the user can corrupt the output if they do.\n\nSurely the close() method is all that is needed?\n",
    "desc_source": "jira"
  },
  "Compress_4": {
    "description": "Are the public finish() methods ArchiveOutputStream implementations necessary and safe?\nSome of the ArchiveOutputStream implementations have public finish() methods. These are currently only called from the close() methods.\n\nSeems to me that there is no need to allow the finish() methods to be called externally, and the user can corrupt the output if they do.\n\nSurely the close() method is all that is needed?\n",
    "desc_source": "jira"
  },
  "Compress_5": {
    "description": "ZipArchiveInputStream doesn't report the end of a truncated archive\nIf a Zip archive is truncated, (e.g. because it is the first volume in a multi-volume archive) the ZipArchiveInputStream.read() method will not detect that fact. All calls to read() will return 0 bytes read. They will not return -1 (end of stream), nor will they throw any exception (which would seem like a good idea to me because the archive is truncated).\n\nI have tracked this problem to ZipArchiveInputStream.java, line 239. It contains a check\n\nif (read == 0 && inf.finished()) {\n    return -1;\n}\n\nFor truncated archives the read is always zero but the inf is never finished(). I suggest adding two lines below:\n\nif (read == 0 && inf.finished()) {\n    return -1;\n} else if (read == 0 && lengthOfLastRead == -1) {\n\tthrow new IOException(\"Truncated ZIP file\");\n}\n\nThis solves the problem in my tests.",
    "desc_source": "jira"
  },
  "Compress_6": {
    "description": "Creating zip files with many entries will ocassionally produce corrupted output\nOur application produces large numbers of zip files, often with 1000's of similarly named files contained within the zip. \nWhen we switched from the standard JDK zip classes to those in commons compress, we would ocassionally produce a zip file that had corrupted index entries and would fail to unzip successfully using 7-zip, winzip, etc.\n\nDebugging the zip creation showed that the the wrong offsets were being returned from the hashmap in ZipOutputStream for the entries that were being corrupted.  Further analysis revealed that this occurred when the filenames being added had a hash collision with another entry in the same output zip (which appears to happen quite frequently for us).\n\nThe issue appears to stem from the fact that ZipArchiveEntry can store the entry name either in its superclass if passed in on the ctor or in its own member attribute if set later via setName().  Not sure whether this functionality is really required?  Regardless, the root cause of the bug is that the equals() and hashCode() methods in ZipArchiveEntry do not always use the same filename value in their comparisons.  In fact if the filename of the entry is set in the ctor it will always treat two ZipArchiveEntries as equal.  This will break the offset hashmap whenever there is a hash collision as it will overwrite the previous entry, believeing it to be equal.\n\nPatch to follow.\n\n",
    "desc_source": "jira"
  },
  "Compress_7": {
    "description": "TarUtils.parseName does not properly handle characters outside the range 0-127\nif a tarfile contains files with special characters, the names of the tar entries are wrong.\n\nexample:\ncorrect name: 0302-0601-3\u00b1\u00b1\u00b1F06\u00b1W220\u00b1ZB\u00b1LALALA\u00b1\u00b1\u00b1\u00b1\u00b1\u00b1\u00b1\u00b1\u00b1\u00b1CAN\u00b1\u00b1DC\u00b1\u00b1\u00b104\u00b1060302\u00b1MOE.model\nname resolved by TarUtils.parseName: 0302-0101-3\uffb1\uffb1\uffb1F06\uffb1W220\uffb1ZB\uffb1HECKMODUL\uffb1\uffb1\uffb1\uffb1\uffb1\uffb1\uffb1\uffb1\uffb1\uffb1ECE\uffb1\uffb1DC\uffb1\uffb1\uffb107\uffb1060302\uffb1DOERN.model\n\nplease use: \nresult.append(new String(new byte[] { buffer[i] }));\n\ninstead of: \nresult.append((char) buffer[i]);\n\nto solve this encoding problem.",
    "desc_source": "jira"
  },
  "Compress_8": {
    "description": "TarArchiveEntry.parseTarHeader() includes the trailing space/NUL when parsing the octal size\nTarArchiveEntry.parseTarHeader() includes the trailing space/NUL when parsing the octal size.\n\nAlthough the size field in the header is 12 bytes, the last byte is supposed to be space or NUL - i.e. only 11 octal digits are allowed for the size.",
    "desc_source": "jira"
  },
  "Compress_9": {
    "description": "TarArchiveOutputStream.getBytesWritten() returns invalid value\nIt appears the TarArchiveOutputStream.getBytesWritten()returns zero or invalid value when queried.\nIn the code sample below, it returns zero, even after an sizeable file was processed.\nI've printed it twice, once before closing the output stream, and once after, just for the reference.\nIt is also demonstrable on multiple processed files.\n\nWithin the TarArchiveOutputStream.getBytesWritten() implementation, it appears the call for count(numToWrite) is made after the numToWrite is depleted in the process of actual byte writing. When call for count(numToWrite); is moved up, the returned values for TarArchiveOutputStream.getBytesWritten() are getting equal to the sum of the sizes of processed files. This is much closer to expected value (\"Returns the current number of bytes written to this stream.\") but still not correct, for that number should include the tar header sizes as well.\n\nAt any rate, please find the proposed patch below, merely moving count(numToWrite); up a few lines. This makes TarArchiveOutputStream.getBytesWritten() closer to true value.\n\n\nTest code:\n{code}\n@Test\n\tpublic void tartest() throws Exception {\n\t\t\n\t\tFileOutputStream myOutputStream = new FileOutputStream(\"C:/temp/tartest.tar\");\n\t\t\n\t\tArchiveOutputStream sTarOut = new ArchiveStreamFactory().createArchiveOutputStream(ArchiveStreamFactory.TAR, myOutputStream);\n\t\t\n\t\tFile sSource = new File(\"C:/share/od_l.txt\");\n\t\tTarArchiveEntry sEntry = new TarArchiveEntry(sSource);\n\t\tsTarOut.putArchiveEntry(sEntry);\n\t\t\n\t\tFileInputStream sInput = new FileInputStream(sSource);\n\t\tbyte[] cpRead = new byte[8192];\n\t\t\n\t\tint iRead = 0;\n\t\twhile ((iRead = sInput.read(cpRead)) > 0) {\n\t\t\tsTarOut.write(cpRead, 0, iRead);\n\t\t}\n\t\t\n\t\tsLog.info(\"Processed: \"+sTarOut.getBytesWritten()+\" bytes. File Len: \"+sSource.length());\n\t\t\n\t\tsInput.close();\n\t\tsTarOut.closeArchiveEntry();\n\t\tsTarOut.close();\n\n\t\tsLog.info(\"Processed: \"+sTarOut.getBytesWritten()+\" bytes. File Len: \"+sSource.length());\n\n\t\t\n\t\treturn;\n\t\t\t\n\t}\n{code}\n\nTest Output:\n{code}\nOct 21, 2011 9:09:28 AM com.cronsult.jndmpd.test.Backup tartest\nINFO: Processed: 0 bytes. File Len: 186974208\nOct 21, 2011 9:09:28 AM com.cronsult.jndmpd.test.Backup tartest\nINFO: Processed: 0 bytes. File Len: 186974208\n{code}\n\nProposed Patch:\n{code}\nIndex: src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveOutputStream.java\n===================================================================\n--- src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveOutputStream.java\t(revision 1187150)\n+++ src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveOutputStream.java\t(working copy)\n@@ -276,6 +276,8 @@\n             // eliminate some of the buffer copying.\n             //\n         }\n+        \n+        count(numToWrite);\n \n         if (assemLen > 0) {\n             if ((assemLen + numToWrite) >= recordBuf.length) {\n@@ -325,7 +327,7 @@\n             wOffset += num;\n         }\n         \n-        count(numToWrite);\n+        \n     }\n \n     /**\n\n{code}",
    "desc_source": "jira"
  },
  "Compress_10": {
    "description": "Cannot Read Winzip Archives With Unicode Extra Fields\nI have a zip file created with WinZip containing Unicode extra fields. Upon attempting to extract it with org.apache.commons.compress.archivers.zip.ZipFile, ZipFile.getInputStream() returns null for ZipArchiveEntries previously retrieved with ZipFile.getEntry() or even ZipFile.getEntries(). See UTF8ZipFilesTest.patch in the attachments for a test case exposing the bug. The original test case stopped short of trying to read the entries, that's why this wasn't flagged up before. \n\nThe problem lies in the fact that inside ZipFile.java entries are stored in a HashMap. However, at one point after populating the HashMap, the unicode extra fields are read, which leads to a change of the ZipArchiveEntry name, and therefore a change of its hash code. Because of this, subsequent gets on the HashMap fail to retrieve the original values.\n\nZipFile.patch contains an (admittedly simple-minded) fix for this problem by reconstructing the entries HashMap after the Unicode extra fields have been parsed. The purpose of this patch is mainly to show that the problem is indeed what I think, rather than providing a well-designed solution.\n\nThe patches have been tested against revision 1210416.",
    "desc_source": "jira"
  },
  "Compress_11": {
    "description": "createArchiveInputStream detects text files less than 100 bytes as tar archives\nThe fix for COMPRESS-117 which modified ArchiveStreamFactory().createArchiveInputStream(inputstream) results in short text files (empirically seems to be those <= 100 bytes) being detected as tar archives which obviously is not desirable if one wants to know whether or not the files are archives.\nI'm not an expert on compressed archives but perhaps the heuristic that if a stream is interpretable as a tar file without an exception being thrown should only be applied on archives greater than 100 bytes?",
    "desc_source": "jira"
  },
  "Compress_12": {
    "description": "TarArchiveInputStream throws IllegalArgumentException instead of IOException\nTarArchiveInputStream is throwing  IllegalArgumentException instead of IOException on corrupt files, in direct contradiction to the Javadoc. Here is a stack-trace:\n\n{code}\njava.lang.IllegalArgumentException: Invalid byte -1 at offset 7 in '<some bytes>' len=8\n\tat org.apache.commons.compress.archivers.tar.TarUtils.parseOctal(TarUtils.java:86)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:790)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.<init>(TarArchiveEntry.java:308)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:198)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextEntry(TarArchiveInputStream.java:380)\n\tat de.schlichtherle.truezip.fs.archive.tar.TarInputShop.<init>(TarInputShop.java:91)\n\tat de.schlichtherle.truezip.fs.archive.tar.TarDriver.newTarInputShop(TarDriver.java:159)\n\tat de.schlichtherle.truezip.fs.archive.tar.TarGZipDriver.newTarInputShop(TarGZipDriver.java:82)\n\tat de.schlichtherle.truezip.fs.archive.tar.TarDriver.newInputShop(TarDriver.java:151)\n\tat de.schlichtherle.truezip.fs.archive.tar.TarDriver.newInputShop(TarDriver.java:47)\n\tat de.schlichtherle.truezip.fs.archive.FsDefaultArchiveController.mount(FsDefaultArchiveController.java:170)\n\tat de.schlichtherle.truezip.fs.archive.FsFileSystemArchiveController$ResetFileSystem.autoMount(FsFileSystemArchiveController.java:98)\n\tat de.schlichtherle.truezip.fs.archive.FsFileSystemArchiveController.autoMount(FsFileSystemArchiveController.java:47)\n\tat de.schlichtherle.truezip.fs.archive.FsArchiveController.autoMount(FsArchiveController.java:129)\n\tat de.schlichtherle.truezip.fs.archive.FsArchiveController.getEntry(FsArchiveController.java:160)\n\tat de.schlichtherle.truezip.fs.archive.FsContextController.getEntry(FsContextController.java:117)\n\tat de.schlichtherle.truezip.fs.FsDecoratingController.getEntry(FsDecoratingController.java:76)\n\tat de.schlichtherle.truezip.fs.FsDecoratingController.getEntry(FsDecoratingController.java:76)\n\tat de.schlichtherle.truezip.fs.FsConcurrentController.getEntry(FsConcurrentController.java:164)\n\tat de.schlichtherle.truezip.fs.FsSyncController.getEntry(FsSyncController.java:108)\n\tat de.schlichtherle.truezip.fs.FsFederatingController.getEntry(FsFederatingController.java:156)\n\tat de.schlichtherle.truezip.nio.file.TFileSystem.newDirectoryStream(TFileSystem.java:348)\n\tat de.schlichtherle.truezip.nio.file.TPath.newDirectoryStream(TPath.java:963)\n\tat de.schlichtherle.truezip.nio.file.TFileSystemProvider.newDirectoryStream(TFileSystemProvider.java:344)\n\tat java.nio.file.Files.newDirectoryStream(Files.java:400)\n\tat com.googlecode.boostmavenproject.GetSourcesMojo.convertToJar(GetSourcesMojo.java:248)\n\tat com.googlecode.boostmavenproject.GetSourcesMojo.download(GetSourcesMojo.java:221)\n\tat com.googlecode.boostmavenproject.GetSourcesMojo.execute(GetSourcesMojo.java:111)\n\tat org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)\n\t... 20 more\n{code}\n\nExpected behavior: TarArchiveInputStream should wrap the IllegalArgumentException in an IOException.",
    "desc_source": "jira"
  },
  "Compress_13": {
    "description": "ArchiveInputStream#getNextEntry(): Problems with WinZip directories with Umlauts\nThere is a problem when handling a WinZip-created zip with Umlauts in directories.\n\nI'm accessing a zip file created with WinZip containing a directory with an umlaut (\"\u00e4\") with ArchiveInputStream. When creating the zip file the unicode-flag of winzip had been active.\n\nThe following problem occurs when accessing the entries of the zip:\nthe ArchiveEntry for a directory containing an umlaut is not marked as a directory and the file names for the directory and all files contained in that directory contain backslashes instead of slashes (i.e. completely different to all other files in directories with no umlaut in their path).\n\nThere is no difference when letting the ArchiveStreamFactory decide which ArchiveInputStream to create or when using the ZipArchiveInputStream constructor with the correct encoding (I've tried different encodings CP437, CP850, ISO-8859-15, but still the problem persisted).\n\nThis problem does not occur when using the very same zip file but compressed by 7zip or the built-in Windows 7 zip functionality.",
    "desc_source": "jira"
  },
  "Compress_14": {
    "description": "Tar files created by AIX native tar, and which contain symlinks, cannot be read by TarArchiveInputStream\nA simple tar file created on AIX using the native ({{/usr/bin/tar}} tar utility) *and* which contains a symbolic link, cannot be loaded by TarArchiveInputStream:\n\n{noformat}\njava.io.IOException: Error detected parsing the header\n\tat org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:201)\n\tat Extractor.extract(Extractor.java:13)\n\tat Extractor.main(Extractor.java:28)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.tools.ant.taskdefs.ExecuteJava.run(ExecuteJava.java:217)\n\tat org.apache.tools.ant.taskdefs.ExecuteJava.execute(ExecuteJava.java:152)\n\tat org.apache.tools.ant.taskdefs.Java.run(Java.java:771)\n\tat org.apache.tools.ant.taskdefs.Java.executeJava(Java.java:221)\n\tat org.apache.tools.ant.taskdefs.Java.executeJava(Java.java:135)\n\tat org.apache.tools.ant.taskdefs.Java.execute(Java.java:108)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:390)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:411)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:809)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)\nCaused by: java.lang.IllegalArgumentException: Invalid byte 0 at offset 0 in '{NUL}1722000726 ' len=12\n\tat org.apache.commons.compress.archivers.tar.TarUtils.parseOctal(TarUtils.java:99)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:819)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.<init>(TarArchiveEntry.java:314)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:199)\n\t... 29 more\n{noformat}\n\nTested with 1.2 and the 1.4 nightly build from Feb 23 ({{Implementation-Build: trunk@r1292625; 2012-02-23 03:20:30+0000}})",
    "desc_source": "jira"
  },
  "Compress_15": {
    "description": "ZipArchiveInputStream and ZipFile don't produce equals ZipArchiveEntry instances\nI'm trying to use a ZipArchiveEntry coming from ZipArchiveInputStream that I stored somwhere for later with a ZipFile and it does not work.\n\nThe reason is that it can't find the ZipArchiveEntry in the ZipFile entries map. It is exactly the same zip file but both entries are not equals so the Map#get fail.\n\nAs far as I can see the main difference is that {{comment}} is null in ZipArchiveInputStream while it's en empty string in ZipFile. I looked at ZipArchiveInputStream and it looks like the comment (whatever it is) is simply not parsed while I can find some code related to the comment at the end of ZIipFile#readCentralDirectoryEntry.\n\nNote that java.util.zip does not have this issue. Did not checked what they do but the zip entries are equals.",
    "desc_source": "jira"
  },
  "Compress_16": {
    "description": "Too relaxed tar detection in ArchiveStreamFactory\nThe relaxed tar detection logic added in COMPRESS-117 unfortunately matches also some non-tar files like a [test AIFF file|https://svn.apache.org/repos/asf/tika/trunk/tika-parsers/src/test/resources/test-documents/testAIFF.aif] that Apache Tika uses. It would be good to improve the detection heuristics to still match files like the one in COMPRESS-117 but avoid false positives like the AIFF file in Tika.",
    "desc_source": "jira"
  },
  "Compress_17": {
    "description": "Tar file for Android backup cannot be read\nAttached tar file was generated by some kind of backup tool on Android. Normal tar utilities seem to handle it fine, but Commons Compress doesn't.\n\n{noformat}\njava.lang.IllegalArgumentException: Invalid byte 0 at offset 5 in '01750{NUL}{NUL}{NUL}' len=8\n    at org.apache.commons.compress.archivers.tar.TarUtils.parseOctal(TarUtils.java:99)\n    at org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:788)\n    at org.apache.commons.compress.archivers.tar.TarArchiveEntry.<init>(TarArchiveEntry.java:308)\n{noformat}\n",
    "desc_source": "jira"
  },
  "Compress_18": {
    "description": "Long directory names can not be stored in a tar archive because of error when writing PAX headers\nTrying to add a directory to the TAR Archive that has a name longer than 100 bytes generates an exception with a stack trace similar to the following:\n\n{noformat}\njava.io.IOException: request to write '114' bytes exceeds size in header of '0' bytes for entry './PaxHeaders.X/layers/openstreetmap__osm.disy.net/.tiles/1.0.0/openstreetmap__osm.disy.net/default/'\n\n            at org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.write(TarArchiveOutputStream.java:385)\n\n            at java.io.OutputStream.write(Unknown Source)\n\n            at org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.writePaxHeaders(TarArchiveOutputStream.java:485)\n\n            at org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.putArchiveEntry(TarArchiveOutputStream.java:312)\n\n            at net.disy.lib.io.tar.TarUtilities.addFile(TarUtilities.java:116)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:158)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:162)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:162)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:162)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:162)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:162)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:162)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:162)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:162)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:162)\n\n            at net.disy.lib.io.tar.TarUtilities.tar(TarUtilities.java:77)\n\n            at net.disy.lib.io.tar.TarUtilities.tar(TarUtilities.java:42)\n\n            at net.disy.gisterm.tilecacheset.export.TileCacheSetExporter.tarTreeStructure(TileCacheSetExporter.java:262)\n\n            at net.disy.gisterm.tilecacheset.export.TileCacheSetExporter.export(TileCacheSetExporter.java:111)\n\n            at net.disy.gisterm.tilecacheset.desktop.controller.ExportController$1.run(ExportController.java:81)\n\n            ... 2 more\n{noformat}\n\nInformal source code investigation points to the problem being that for directory entries the code assumes that the length is 0 in putArchiveEntry (see TarArchiveOutputStream:321 ) but when writing the data, it actually writes some data (the filename) and the length written (filename size) is larger than the length expected (0).",
    "desc_source": "jira"
  },
  "Compress_19": {
    "description": "ZipException on reading valid zip64 file\nZipFile zip = new ZipFile(new File(\"ordertest-64.zip\")); throws ZipException \"central directory zip64 extended information extra field's length doesn't match central directory data.  Expected length 16 but is 28\".\n\nThe archive was created by using DotNetZip-WinFormsTool uzing zip64 flag (forces always to make zip64 archives).\n\nZip file is tested from the console: $zip -T ordertest-64.zip\n\nOutput:\ntest of ordertest-64.zip OK\n\nI can open the archive with FileRoller without problem on my machine, browse and extract it.\n",
    "desc_source": "jira"
  },
  "Compress_20": {
    "description": "IllegalArgumentException reading CPIO generated by Redline RPM\nhttp://redline-rpm.org/ creates CPIO archives with a non-zero file mode on the trailer. This causes an IllegalArgumentException when reading the file. I've attached a patch and test archive to fix this.",
    "desc_source": "jira"
  },
  "Compress_21": {
    "description": "Writing 7z empty entries produces incorrect or corrupt archive\nI couldn't find an exact rule that causes this incorrect behavior, but I tried to reduce it to some simple scenarios to reproduce it:\n\nInput: A folder with certain files -> tried to archive it.\nIf the folder contains more than 7 files the incorrect behavior appears.\n\nScenario 1: 7 empty files\nResult: The created archive contains a single folder entry with the name of the archive (no matter which was the name of the file)\n\nScenario 2: 7 files, some empty, some with content\nResult: The created archive contains a folder entry with the name of the archive and a number of file entries also with the name of the archive. The number of the entries is equal to the number of non empty files.\n\nScenario 3: 8 empty files\nResult: 7zip Manager cannot open archive and stops working.\n\nScenario 4.1: 8 files: some empty, some with content, last file (alphabetically) with content\nResult: same behavior as described for Scenario 2.\n\nScenario 4.2: 8 files, some empty, some with content, last file empy\nResult: archive is corrupt, the following message is received: \"Cannot open file 'archivename.7z' as archive\" (7Zip Manager does not crash).",
    "desc_source": "jira"
  },
  "Compress_22": {
    "description": "BZip2CompressorInputStream reads fewer bytes from truncated file than CPython's bz2 implementation\nJython includes support for decompressing bz2 files using commons compress and shares regression tests with CPython. The CPython test [test_read_truncated|https://bitbucket.org/jython/jython/src/b2890af7a5e817e30f6ca2325f6dcdb14a59f32b/lib-python/2.7/test/test_bz2.py?at=default#cl-331] in test_bz2.py passes under CPython but fails under Jython.\n\nThe BZip2CompressorInputStream is able to read 769 bytes from the truncated data rather than the 770 bytes that the CPython bz2 implementation can read.",
    "desc_source": "jira"
  },
  "Compress_23": {
    "description": "7z: 16 MB dictionary is too big\nI created an archiv with 7zip 9.20 containing the compress-1.7-src directory. Also tried it with 1.6 version and directory. I \n\ndownloaded the zip file and reziped it as 7z. The standard setting where used:\nCompression level: normal\nCompression method: lzma2\nDictionary size: 16 MB\nWord size: 32\nSolid Block size: 2 GB\n\nI get an exception if I try to open the file with the simple line of code:\nSevenZFile input = new SevenZFile(new File(arcName));\n\nMaybe it is a bug in the tukaani library, but I do not know how to report it to them.\nThe exception thrown:\n\norg.tukaani.xz.UnsupportedOptionsException: LZMA dictionary is too big for this implementation\n\tat org.tukaani.xz.LZMAInputStream.initialize(Unknown Source)\n\tat org.tukaani.xz.LZMAInputStream.<init>(Unknown Source)\n\tat org.apache.commons.compress.archivers.sevenz.Coders$LZMADecoder.decode(Coders.java:117)\n\tat org.apache.commons.compress.archivers.sevenz.Coders.addDecoder(Coders.java:48)\n\tat org.apache.commons.compress.archivers.sevenz.SevenZFile.readEncodedHeader(SevenZFile.java:278)\n\tat org.apache.commons.compress.archivers.sevenz.SevenZFile.readHeaders(SevenZFile.java:190)\n\tat org.apache.commons.compress.archivers.sevenz.SevenZFile.<init>(SevenZFile.java:94)\n\tat org.apache.commons.compress.archivers.sevenz.SevenZFile.<init>(SevenZFile.java:116)\n\tat compress.SevenZipError.main(SevenZipError.java:28)",
    "desc_source": "jira"
  },
  "Compress_24": {
    "description": "TarArchiveInputStream fails to read entry with big user-id value\nCaused by: java.lang.IllegalArgumentException: Invalid byte 52 at offset 7 in '62410554' len=8\n\tat org.apache.commons.compress.archivers.tar.TarUtils.parseOctal(TarUtils.java:130)\n\tat org.apache.commons.compress.archivers.tar.TarUtils.parseOctalOrBinary(TarUtils.java:175)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:953)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:940)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.<init>(TarArchiveEntry.java:324)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:247)\n\t... 5 more",
    "desc_source": "jira"
  },
  "Compress_25": {
    "description": "ZIP reads correctly with commons-compress 1.6, gives NUL bytes in 1.7\nWhen running the code below, commons-compress 1.6 writes:\n\n Content of test.txt:\n data\n\nBy comparison, commons-compress 1.7 writes\n\n Content of test.txt:\n ^@^@^@^@^@\n\npackage com.example.jrn;\nimport org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\nimport org.apache.commons.compress.archivers.zip.ZipArchiveInputStream;\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.lang.System;\n/**\n * Hello world!\n *\n */\npublic class App {\n  public static void main(String[] args) {\n    byte[] zip = {\n       (byte)0x50, (byte)0x4b, (byte)0x03, (byte)0x04, (byte)0x0a, (byte)0x00,\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x03, (byte)0x7b,\n       (byte)0xd1, (byte)0x42, (byte)0x82, (byte)0xc5, (byte)0xc1, (byte)0xe6,\n       (byte)0x05, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x05, (byte)0x00,\n       (byte)0x00, (byte)0x00, (byte)0x08, (byte)0x00, (byte)0x1c, (byte)0x00,\n       (byte)0x74, (byte)0x65, (byte)0x73, (byte)0x74, (byte)0x2e, (byte)0x74,\n       (byte)0x78, (byte)0x74, (byte)0x55, (byte)0x54, (byte)0x09, (byte)0x00,\n       (byte)0x03, (byte)0x56, (byte)0x62, (byte)0xbf, (byte)0x51, (byte)0x2a,\n       (byte)0x63, (byte)0xbf, (byte)0x51, (byte)0x75, (byte)0x78, (byte)0x0b,\n       (byte)0x00, (byte)0x01, (byte)0x04, (byte)0x01, (byte)0xff, (byte)0x01,\n       (byte)0x00, (byte)0x04, (byte)0x88, (byte)0x13, (byte)0x00, (byte)0x00,\n       (byte)0x64, (byte)0x61, (byte)0x74, (byte)0x61, (byte)0x0a, (byte)0x50,\n       (byte)0x4b, (byte)0x01, (byte)0x02, (byte)0x1e, (byte)0x03, (byte)0x0a,\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x03,\n       (byte)0x7b, (byte)0xd1, (byte)0x42, (byte)0x82, (byte)0xc5, (byte)0xc1,\n       (byte)0xe6, (byte)0x05, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x05,\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x08, (byte)0x00, (byte)0x18,\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x01,\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0xa0, (byte)0x81, (byte)0x00,\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x74, (byte)0x65, (byte)0x73,\n       (byte)0x74, (byte)0x2e, (byte)0x74, (byte)0x78, (byte)0x74, (byte)0x55,\n       (byte)0x54, (byte)0x05, (byte)0x00, (byte)0x03, (byte)0x56, (byte)0x62,\n       (byte)0xbf, (byte)0x51, (byte)0x75, (byte)0x78, (byte)0x0b, (byte)0x00,\n       (byte)0x01, (byte)0x04, (byte)0x01, (byte)0xff, (byte)0x01, (byte)0x00,\n       (byte)0x04, (byte)0x88, (byte)0x13, (byte)0x00, (byte)0x00, (byte)0x50,\n       (byte)0x4b, (byte)0x05, (byte)0x06, (byte)0x00, (byte)0x00, (byte)0x00,\n       (byte)0x00, (byte)0x01, (byte)0x00, (byte)0x01, (byte)0x00, (byte)0x4e,\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x47, (byte)0x00, (byte)0x00,\n       (byte)0x00, (byte)0x00, (byte)00\n    };\n\n    ByteArrayInputStream bin = new ByteArrayInputStream(zip);\n    try {\n      ZipArchiveInputStream in = new ZipArchiveInputStream(bin);\n      try {\n        while (true) {\n          ZipArchiveEntry entry = in.getNextZipEntry();\n          if (entry == null) {\n            break;\n          }\n          byte[] buf = new byte[(int) entry.getSize()];\n          in.read(buf);\n          System.out.println(\"Content of \" + entry.getName() + \":\");\n          System.out.write(buf);\n        }\n      } finally {\n        in.close();\n      }\n    } catch (IOException e) {\n      System.err.println(\"IOException: \" + e);\n    }\n  }\n}",
    "desc_source": "jira"
  },
  "Compress_26": {
    "description": "IOUtils.skip does not work as advertised\nI am trying to feed a TarInputStream from a CipherInputStream.\nIt does not work, because IOUtils.skip() does not adhere to the contract it claims in javadoc:\n\n\"     * <p>This method will only skip less than the requested number of\n     * bytes if the end of the input stream has been reached.</p>\"\n\nHowever it does:\n\n            long skipped = input.skip(numToSkip);\n            if (skipped == 0) {\n                break;\n            }\n\nAnd the input stream javadoc says:\n\n\"     * This may result from any of a number of conditions; reaching end of file\n     * before <code>n</code> bytes have been skipped is only one possibility.\"\n\nIn the case of CipherInputStream, it stops at the end of each byte buffer.\n\nIf you check the IOUtils from colleagues at commons-io, they have considered this case in IOUtils.skip() where they use a read to skip through the stream.\nAn optimized version could combine trying to skip, then read then trying to skip again.",
    "desc_source": "jira"
  },
  "Compress_27": {
    "description": "Incorrect handling of NUL username and group Tar.gz entries\nWith version 1.8 of commons-compress it's no longer possible to decompress  files from an archive if the archive contains entries having null (or being empty?) set as username and/or usergroup. With version 1.7 this still worked now I get this exception:\n\n{code}\njava.io.IOException: Error detected parsing the header\n\tat org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:249)\n\tat TestBed.AppTest.extractNoFileOwner(AppTest.java:30)\nCaused by: java.lang.IllegalArgumentException: Invalid byte 32 at offset 7 in '       {NUL}' len=8\n\tat org.apache.commons.compress.archivers.tar.TarUtils.parseOctal(TarUtils.java:134)\n\tat org.apache.commons.compress.archivers.tar.TarUtils.parseOctalOrBinary(TarUtils.java:173)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:953)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:940)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.<init>(TarArchiveEntry.java:324)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:247)\n\t... 27 more\n\n{code}\nThis exception leads to my suspision that the regression was introduced with the fix for this ticket COMPRESS-262, which has a nearly identical exception provided.\n\nSome test code you can run to verify it:\n\n{code}\npackage TestBed;\n\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileNotFoundException;\nimport java.io.IOException;\n\nimport org.apache.commons.compress.archivers.tar.TarArchiveEntry;\nimport org.apache.commons.compress.archivers.tar.TarArchiveInputStream;\nimport org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream;\nimport org.junit.Test;\n\n/**\n * Unit test for simple App.\n */\npublic class AppTest\n{\n\n    @Test\n    public void extractNoFileOwner()\n    {\n        TarArchiveInputStream tarInputStream = null;\n\n        try\n        {\n            tarInputStream =\n                new TarArchiveInputStream( new GzipCompressorInputStream( new FileInputStream( new File(\n                    \"/home/pknobel/redis-dist-2.8.3_1-linux.tar.gz\" ) ) ) );\n            TarArchiveEntry entry;\n            while ( ( entry = tarInputStream.getNextTarEntry() ) != null )\n            {\n                System.out.println( entry.getName() );\n                System.out.println(entry.getUserName()+\"/\"+entry.getGroupName());\n            }\n\n        }\n        catch ( FileNotFoundException e )\n        {\n            e.printStackTrace();\n        }\n        catch ( IOException e )\n        {\n            e.printStackTrace();\n        }\n    }\n\n}\n{code}\nWith 1.7 the TestCase outputed this:\n\n{code}\nredis-dist-2.8.3_1/bin/\n/\nredis-dist-2.8.3_1/bin/redis-server\njenkins/jenkins\nredis-dist-2.8.3_1/bin/redis-cli\njenkins/jenkins\n{code}\n\nWith 1.8 it's failing once it reaches the null valued entry, which is the first. The archive is created using maven assembly plugin, and I tried the same with maven ant task. Both generating an archive with not set username and groups for at least some entries.\n\nYou can download the archive from http://heli0s.darktech.org/redis/2.8.3_1/redis-dist-2.8.3_1-linux.tar.gz\n\nIf you run a tar -tvzf on the file you see this report:\n\n{code}\ndrwxr-xr-x 0/0               0 2014-04-18 09:43 redis-dist-2.8.3_1-SNAPSHOT/bin/\n-rwxr-xr-x pknobel/pknobel 3824588 2014-01-02 14:58 redis-dist-2.8.3_1-SNAPSHOT/bin/redis-cli\n-rwxr-xr-x pknobel/pknobel 5217234 2014-01-02 14:58 redis-dist-2.8.3_1-SNAPSHOT/bin/redis-server\n{code}\n\nThe user 0/0 probably indicates that it's not set although it's the root user id. A correctly root user file would show up as root/root",
    "desc_source": "jira"
  },
  "Compress_28": {
    "description": "TarArchiveInputStream silently finished when unexpected EOF occured\nI just found the following test case didn't raise an IOException as it used to be for a *tar trimmed on purpose* \n\n@Test\n  public void testCorruptedBzip2() throws IOException {\n    String archivePath = PathUtil.join(testdataDir, \"test.tar.bz2\");\n    TarArchiveInputStream input = null;\n    input = new TarArchiveInputStream(new BZip2CompressorInputStream(\n        GoogleFile.SYSTEM.newInputStream(archivePath), true));\n    ArchiveEntry nextMatchedEntry = input.getNextEntry();\n    while (nextMatchedEntry != null) {\n      logger.infofmt(\"Extracting %s\", nextMatchedEntry.getName());\n      String outputPath = PathUtil.join(\"/tmp/\", nextMatchedEntry.getName());\n      OutputStream out = new FileOutputStream(outputPath);\n      ByteStreams.copy(input, out);\n      out.close();\n      nextMatchedEntry = input.getNextEntry();\n    }\n  }",
    "desc_source": "jira"
  },
  "Compress_29": {
    "description": "ArchiveStreamFactory fails to pass on the encoding when creating some streams\nArchiveStreamFactory fails to pass on the encoding when creating the following streams (in some or all cases):\n* ArjArchiveInputStream\n* CpioArchiveInputStream\n* DumpArchiveInputStream\n* JarArchiveInputStream\n* JarArchiveOutputStream",
    "desc_source": "jira"
  },
  "Compress_30": {
    "description": "BZip2CompressorInputStream return value wrong when told to read to a full buffer.\nBZip2CompressorInputStream.read(buffer, offset, length) returns -1 when given an offset equal to the length of the buffer.\n\nThis indicates, not that the buffer was full, but that the stream was finished.\n\nIt seems like a pretty stupid thing to do - but I'm getting this when trying to use Kryo serialization (which is probably a bug on their part, too), so it does occur and has negative affects.\n\nHere's a JUnit test that shows the problem specifically:\n\n{noformat}\n\t@Test\n\tpublic void testApacheCommonsBZipUncompression () throws Exception {\n\t\t// Create a big random piece of data\n\t\tbyte[] rawData = new byte[1048576];\n\t\tfor (int i=0; i<rawData.length; ++i) {\n\t\t\trawData[i] = (byte) Math.floor(Math.random()*256);\n\t\t}\n\n\t\t// Compress it\n\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n\t\tBZip2CompressorOutputStream bzipOut = new BZip2CompressorOutputStream(baos);\n\t\tbzipOut.write(rawData);\n\t\tbzipOut.flush();\n\t\tbzipOut.close();\n\t\tbaos.flush();\n\t\tbaos.close();\n\n\t\t// Try to read it back in\n\t\tByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());\n\t\tBZip2CompressorInputStream bzipIn = new BZip2CompressorInputStream(bais);\n\t\tbyte[] buffer = new byte[1024];\n\t\t// Works fine\n\t\tAssert.assertEquals(1024, bzipIn.read(buffer, 0, 1024));\n\t\t// Fails, returns -1 (indicating the stream is complete rather than that the buffer \n\t\t// was full)\n\t\tAssert.assertEquals(0, bzipIn.read(buffer, 1024, 0));\n\t\t// But if you change the above expected value to -1, the following line still works\n\t\tAssert.assertEquals(1024, bzipIn.read(buffer, 0, 1024));\n\t\tbzipIn.close();\n\t}\n{noformat}\n",
    "desc_source": "jira"
  },
  "Compress_31": {
    "description": "Illegal argument exception when extracting .tgz file \nWhen attempting to unpack a .tgz file, I am receiving the illegal argument exception: java.lang.IllegalArgumentException: Invalid byte 0 at offset 5 in '05412{NUL}11' len=8. This is causing a java.io.IOException: Error detected parsing the header error. \n\nThis is being thrown when the function TarArchiveInputStream.getNextTarEntry() is called. \n\nHere is the code I am using. \n\n{code:java}\n            TarArchiveInputStream tarIn = new TarArchiveInputStream(\n                    new GZIPInputStream(\n                            new BufferedInputStream(\n                                    new FileInputStream(\n                                            tempDirPath + fileName))));\n\n            TarArchiveEntry entry = tarIn.getNextTarEntry();\n\n            while (entry != null) {\n                File path = new File(tempDirPath, entry.getName());\n                if (entry.isDirectory()) {\n                    path.mkdirs();\n                } else {          \n                    path.createNewFile();\n                    byte[] read = new byte[2048];\n                    BufferedOutputStream bout = new BufferedOutputStream(new FileOutputStream(path));\n                    int len;\n                    while ((len = tarIn.read(read)) != -1) {\n                        bout.write(read, 0, len);\n                        System.out.print(new String(read, \"UTF-8\"));\n                    }\n                    bout.close();\n                    read = null;\n                }\n                entry = tarIn.getNextTarEntry();\n            }\n            tarIn.close();\n{code}\n\nHere is the full stack trace: \n\n[2015-02-12T23:17:31.944+0000] [glassfish 4.0] [SEVERE] [] [] [tid: _ThreadID=123 _ThreadName=Thread-4] [timeMillis: 1423783051944] [levelValue: 1000] [[\n  java.io.IOException: Error detected parsing the header\n        at org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:257)\n        at org.unavco.ws.tilt.ExtractTiltFile.extractFile(ExtractTiltFile.java:125)\n        at org.unavco.ws.tilt.ExtractTiltFile.run(ExtractTiltFile.java:59)\n        at org.unavco.ws.cache.ProcessDataFile.getFileData(ProcessDataFile.java:100)\n        at org.unavco.ws.cache.ProcessDataFile.getResultSet(ProcessDataFile.java:81)\n        at org.unavco.ws.tilt.TiltDsClient.write(TiltDsClient.java:47)\n        at org.glassfish.jersey.message.internal.StreamingOutputProvider.writeTo(StreamingOutputProvider.java:76)\n        at org.glassfish.jersey.message.internal.StreamingOutputProvider.writeTo(StreamingOutputProvider.java:58)\n        at org.glassfish.jersey.message.internal.WriterInterceptorExecutor$TerminalWriterInterceptor.aroundWriteTo(WriterInterceptorExecutor.java:194)\n        at org.glassfish.jersey.message.internal.WriterInterceptorExecutor.proceed(WriterInterceptorExecutor.java:139)\n        at org.glassfish.jersey.server.internal.JsonWithPaddingInterceptor.aroundWriteTo(JsonWithPaddingInterceptor.java:103)\n        at org.glassfish.jersey.message.internal.WriterInterceptorExecutor.proceed(WriterInterceptorExecutor.java:139)\n        at org.glassfish.jersey.server.internal.MappableExceptionWrapperInterceptor.aroundWriteTo(MappableExceptionWrapperInterceptor.java:88)\n        at org.glassfish.jersey.message.internal.WriterInterceptorExecutor.proceed(WriterInterceptorExecutor.java:139)\n        at org.glassfish.jersey.message.internal.MessageBodyFactory.writeTo(MessageBodyFactory.java:1005)\n        at org.glassfish.jersey.server.ServerRuntime$Responder.writeResponse(ServerRuntime.java:471)\n        at org.glassfish.jersey.server.ServerRuntime$Responder.processResponse(ServerRuntime.java:333)\n        at org.glassfish.jersey.server.ServerRuntime$Responder.process(ServerRuntime.java:323)\n        at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:227)\n        at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)\n        at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)\n        at org.glassfish.jersey.internal.Errors.process(Errors.java:315)\n        at org.glassfish.jersey.internal.Errors.process(Errors.java:297)\n        at org.glassfish.jersey.internal.Errors.process(Errors.java:267)\n        at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:317)\n        at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:198)\n        at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:946)\n        at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:323)\n        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:372)\n        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:335)\n        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:218)\n        at org.apache.catalina.core.StandardWrapper.service(StandardWrapper.java:1682)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:344)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:214)\n        at com.thetransactioncompany.cors.CORSFilter.doFilter(Unknown Source)\n        at com.thetransactioncompany.cors.CORSFilter.doFilter(Unknown Source)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:256)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:214)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:316)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:160)\n        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:734)\n        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:673)\n        at com.sun.enterprise.web.WebPipeline.invoke(WebPipeline.java:99)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:174)\n        at org.apache.catalina.connector.CoyoteAdapter.doService(CoyoteAdapter.java:357)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:260)\n        at com.sun.enterprise.v3.services.impl.ContainerMapper.service(ContainerMapper.java:188)\n        at org.glassfish.grizzly.http.server.HttpHandler.runService(HttpHandler.java:191)\n        at org.glassfish.grizzly.http.server.HttpHandler.doHandle(HttpHandler.java:168)\n        at org.glassfish.grizzly.http.server.HttpServerFilter.handleRead(HttpServerFilter.java:189)\n        at org.glassfish.grizzly.filterchain.ExecutorResolver$9.execute(ExecutorResolver.java:119)\n        at org.glassfish.grizzly.filterchain.DefaultFilterChain.executeFilter(DefaultFilterChain.java:288)\n        at org.glassfish.grizzly.filterchain.DefaultFilterChain.executeChainPart(DefaultFilterChain.java:206)\n        at org.glassfish.grizzly.filterchain.DefaultFilterChain.execute(DefaultFilterChain.java:136)\n        at org.glassfish.grizzly.filterchain.DefaultFilterChain.process(DefaultFilterChain.java:114)\n        at org.glassfish.grizzly.ProcessorExecutor.execute(ProcessorExecutor.java:77)\n        at org.glassfish.grizzly.nio.transport.TCPNIOTransport.fireIOEvent(TCPNIOTransport.java:838)\n        at org.glassfish.grizzly.strategies.AbstractIOStrategy.fireIOEvent(AbstractIOStrategy.java:113)\n        at org.glassfish.grizzly.strategies.WorkerThreadIOStrategy.run0(WorkerThreadIOStrategy.java:115)\n        at org.glassfish.grizzly.strategies.WorkerThreadIOStrategy.access$100(WorkerThreadIOStrategy.java:55)\n        at org.glassfish.grizzly.strategies.WorkerThreadIOStrategy$WorkerThreadRunnable.run(WorkerThreadIOStrategy.java:135)\n        at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:564)\n        at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.run(AbstractThreadPool.java:544)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalArgumentException: Invalid byte 0 at offset 5 in '05412{NUL}11' len=8\n        at org.apache.commons.compress.archivers.tar.TarUtils.parseOctal(TarUtils.java:138)\n        at org.apache.commons.compress.archivers.tar.TarUtils.parseOctalOrBinary(TarUtils.java:169)\n        at org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:951)\n        at org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:940)\n        at org.apache.commons.compress.archivers.tar.TarArchiveEntry.<init>(TarArchiveEntry.java:324)\n        at org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:255)\n        ... 63 more]]\n\n\n",
    "desc_source": "jira"
  },
  "Compress_32": {
    "description": "TarArchiveInputStream rejects uid or gid >= 0x80000000\nA POSIX-format archive that came from sysdiagnose produces NumberFormatException[1] when I try to read it with TarArchiveInputStream.\n\nThe relevant part of the .tar file looks like this:\n\n   18 uid=429496729\n\nThat's the uid of 'nobody' on Mac OS (on Mac OS, uid_t is 'unsigned int').\n\nPOSIX doesn't say anything about the width of the uid extended header[2], so I assume the tar file is okay. GNU tar doesn't have trouble with it.\n\nThe relevant code, in applyPaxHeadersToCurrentEntry:\n\n            } else if (\"gid\".equals(key)){\n                currEntry.setGroupId(Integer.parseInt(val));\n...\n            } else if (\"uid\".equals(key)){\n                currEntry.setUserId(Integer.parseInt(val));\n\nuid_t and gid_t are typically unsigned 32-bit integers, so these should presumably use Long.parseLong to handle integers with the top bit set (and TarArchiveEntry would need some modifications to handle large uid and gid, too).\n\n[1] java.lang.NumberFormatException: For input string: \"4294967294\"\n        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n        at java.lang.Integer.parseInt(Integer.java:495)\n        at java.lang.Integer.parseInt(Integer.java:527)\n        at org.apache.commons.compress.archivers.tar.TarArchiveInputStream.applyPaxHeadersToCurrentEntry(TarArchiveInputStream.java:488)\n        at org.apache.commons.compress.archivers.tar.TarArchiveInputStream.paxHeaders(TarArchiveInputStream.java:415)\n        at org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:295)\n\n[2] http://pubs.opengroup.org/onlinepubs/9699919799/utilities/pax.html#tag_20_92_13_03\nuid\nThe user ID of the file owner, expressed as a decimal number using digits from the ISO/IEC 646:1991 standard. This record shall override the uid field in the following header block(s). When used in write or copy mode, pax shall include a uid extended header record for each file whose owner ID is greater than 2097151 (octal 7777777).",
    "desc_source": "jira"
  },
  "Compress_33": {
    "description": "CompressorStreamFactory doesn't handle deflate streams with a zlib header\nIf you take a zlib / deflate compressed file, with the zlib header (eg the test file bla.tar.deflatez) and pass it to CompressorStreamFactory.createCompressorInputStream, it won't be detected and you'll get a CompressorException(\"No Compressor found for the stream signature.\")\n\nWhile detecting header-less zlib files is probably too tricky to manage, those with the header ought to be possible to spot and handle",
    "desc_source": "jira"
  },
  "Compress_34": {
    "description": "Exception in X7875_NewUnix.parseFromLocalFileData when parsing 0-sized \"ux\" local entry\nWhen trying to detect content type of a zip file with Tika 1.10 (which uses Commons Compress 1.9 internally) in manner like this:\n\n{code}\n        byte[] content = ... // whole zip file.\n        String name = \"TR_01.ZIP\";\n        Tika tika = new Tika();\n        return tika.detect(content, name);\n{code}\n\nit throws an exception:\n\n{code}\njava.lang.ArrayIndexOutOfBoundsException: 13\n\tat org.apache.commons.compress.archivers.zip.X7875_NewUnix.parseFromLocalFileData(X7875_NewUnix.java:199)\n\tat org.apache.commons.compress.archivers.zip.X7875_NewUnix.parseFromCentralDirectoryData(X7875_NewUnix.java:220)\n\tat org.apache.commons.compress.archivers.zip.ExtraFieldUtils.parse(ExtraFieldUtils.java:174)\n\tat org.apache.commons.compress.archivers.zip.ZipArchiveEntry.setCentralDirectoryExtra(ZipArchiveEntry.java:476)\n\tat org.apache.commons.compress.archivers.zip.ZipFile.readCentralDirectoryEntry(ZipFile.java:575)\n\tat org.apache.commons.compress.archivers.zip.ZipFile.populateFromCentralDirectory(ZipFile.java:492)\n\tat org.apache.commons.compress.archivers.zip.ZipFile.<init>(ZipFile.java:216)\n\tat org.apache.commons.compress.archivers.zip.ZipFile.<init>(ZipFile.java:192)\n\tat org.apache.commons.compress.archivers.zip.ZipFile.<init>(ZipFile.java:153)\n\tat org.apache.tika.parser.pkg.ZipContainerDetector.detectZipFormat(ZipContainerDetector.java:141)\n\tat org.apache.tika.parser.pkg.ZipContainerDetector.detect(ZipContainerDetector.java:88)\n\tat org.apache.tika.detect.CompositeDetector.detect(CompositeDetector.java:77)\n\tat org.apache.tika.Tika.detect(Tika.java:155)\n\tat org.apache.tika.Tika.detect(Tika.java:183)\n\tat org.apache.tika.Tika.detect(Tika.java:223)\n{code}\n\nThe zip file does contain two .jpg images and is not a \"special\" (JAR, Openoffice, ... ) zip file.\n\nUnfortunately, the contents of the zip file is confidential and so I cannot attach it to this ticket as it is, although I can provide the parameters supplied to\norg.apache.commons.compress.archivers.zip.X7875_NewUnix.parseFromLocalFileData(X7875_NewUnix.java:199) as caught by the debugger:\n\n{code}\ndata = {byte[13]@2103}\n 0 = 85\n 1 = 84\n 2 = 5\n 3 = 0\n 4 = 7\n 5 = -112\n 6 = -108\n 7 = 51\n 8 = 85\n 9 = 117\n 10 = 120\n 11 = 0\n 12 = 0\noffset = 13\nlength = 0\n{code}\n\nThis data comes from the local zip entry for the first file, it seems the method tries to read more bytes than is actually available in the buffer.\n\nIt seems that first 9 bytes of the buffer are 'UT' extended field with timestamp, followed by 0-sized 'ux' field (bytes 9-12) that is supposed to contain UID/GID - according to infozip's doc the 0-size is common for global dictionary, but the local dictionary should contain complete data. In this case for some reason it does contain 0-sized data.\n\nNote that 7zip and unzip can unzip the file without even a warning, so Commons Compress should be also able to handle that file correctly without choking on that exception.",
    "desc_source": "jira"
  },
  "Compress_35": {
    "description": "TAR checksum fails when checksum is right aligned\nThe linked TAR has a checksum with zero padding on the left instead of the expected {{NULL-SPACE}} terminator on the right. As a result the last two digits of the stored checksum are lost and the otherwise valid checksum is treated as invalid.\n\nGiven that the code already checks for digits being in range before adding them to the stored sum, is it necessary to only look at the first 6 octal digits instead of the whole field?",
    "desc_source": "jira"
  },
  "Compress_36": {
    "description": "Calling SevenZFile.read() on empty SevenZArchiveEntry throws IllegalStateException\nI'm pretty sure COMPRESS-340 breaks reading empty archive entries. When calling getNextEntry() and that entry has no content, the code jumps into the first block at line 830 (SevenZFile.class), clearing the deferredBlockStreams. When calling entry.read(...) afterwards an IllegalStateException (\"No current 7z entry (call getNextEntry() first).\") is thrown. IMHO, there should be another check for entry.getSize() == 0.\n\nThis worked correctly up until 1.10.\n",
    "desc_source": "jira"
  },
  "Compress_37": {
    "description": "Parsing PAX headers fails with NegativeArraySizeException\nThe {{TarArchiveInputStream.parsePaxHeaders}} method fails with a {{NegativeArraySizeException}} when there is an empty line at the end of the headers.\n\nThe inner loop starts reading the length, but it gets a newline (10) and ends up subtracting '0' (48) from it; the result is a negative length that blows up an attempt to allocate the {{rest}} array.\n\nI would say that a check to see if {{ch}} is less the '0' and break the loop if it is.\n\nI used {{npm pack aws-sdk@2.2.16}} to generate a tarball with this issue.",
    "desc_source": "jira"
  },
  "Compress_38": {
    "description": "PAX header entry name ending with / causes problems\nThere seems to be a problem when a PAX header entry (link flag is 'x') has a name ending with \"/\". The {{TarArchiveEntry.isDirectory()}} check ends up returning {{true}} because of the trailing slash which means no content can be read from the entry. PAX header parsing effectively finds nothing and the stream is not advanced; this leaves the stream in a bad state as the next entry's header is actually read from the header contents.\n\nIf the name is modified to remove the trailing slash when the link flag indicates a PAX header everything seems to work fine. That would be one potential fix in {{parseTarHeader}}. Changing {{isDirectory}} to return {{false}} if {{isPaxHeader}} is {{true}} (before the trailing \"/\" check) would probably also fix the issue (though I can't verify that in the debugger like I can with changing the name).\n\nSo far I have only seen this when using Docker to save images that contain a yum database. For example:\n{noformat}\ndocker pull centos:latest && docker save centos:latest | tar x --include \"*/layer.tar\"\n{noformat}\nWill produce at least one \"layer.tar\" that exhibits this issue. If I come across a smaller TAR for testing I will attach it.",
    "desc_source": "jira"
  },
  "Compress_39": {
    "description": "Defective .zip-archive produces problematic error message\nA truncated .zip-File produces an java.io.EOFException conatining a hughe amount of byte[]-data in the error-message - leading to beeps and crippeling workload in an potential console-logger.\n\n",
    "desc_source": "jira"
  },
  "Compress_40": {
    "description": "Overflow in BitInputStream\nin Class BitInputStream.java(\\src\\main\\java\\org\\apache\\commons\\compress\\utils),\nfuncion:\n\n public long readBits(final int count) throws IOException {\n        if (count < 0 || count > MAXIMUM_CACHE_SIZE) {\n            throw new IllegalArgumentException(\"count must not be negative or greater than \" + MAXIMUM_CACHE_SIZE);\n        }\n        while (bitsCachedSize < count) {\n            final long nextByte = in.read();\n            if (nextByte < 0) {\n                return nextByte;\n            }\n            if (byteOrder == ByteOrder.LITTLE_ENDIAN) {\n                bitsCached |= (nextByte << bitsCachedSize);\n            } else {\n                bitsCached <<= 8;\n                bitsCached |= nextByte;\n            }\n            bitsCachedSize += 8;\n        }\n\n        final long bitsOut;\n        if (byteOrder == ByteOrder.LITTLE_ENDIAN) {\n            bitsOut = (bitsCached & MASKS[count]);\n            bitsCached >>>= count;\n        } else {\n            bitsOut = (bitsCached >> (bitsCachedSize - count)) & MASKS[count];\n        }\n        bitsCachedSize -= count;\n        return bitsOut;\n    }\n\n\n\nI think here \"bitsCached |= (nextByte << bitsCachedSize);\" will overflow in some cases. for example, below is a test case:\n\npublic static void test() {\n\n        ByteArrayInputStream in = new ByteArrayInputStream(new byte[]{87, 45, 66, 15,\n                                                                      90, 29, 88, 61, 33, 74});\n        BitInputStream bin = new BitInputStream(in, ByteOrder.LITTLE_ENDIAN);\n        try {\n            long ret = bin.readBits(5);\n            ret = bin.readBits(63);\n            ret = bin.readBits(12);\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n}\n\noverflow occur in \"bin.readBits(63);\" , so ,result in wrong result from  \"bin.readBits(12);\" \n\n",
    "desc_source": "jira"
  },
  "Compress_41": {
    "description": "ZipArchiveInputStream.getNextZipEntry() should differentiate between \"invalid entry encountered\" and \"no more entries\"\nZipArchiveInputStream.getNextZipEntry() currently returns null if an invalid entry is encountered.  Thus, it's not possible to differentiate between \"no more entries\" and \"invalid entry encountered\" conditions.\n\nInstead, it should throw an exception if an invalid entry is encountered.\n\nI've created a test case and fix. I will submit a pull request shortly.",
    "desc_source": "jira"
  },
  "Compress_42": {
    "description": "isUnixSymlink returns true for Zip entries with Unix permissions 177777\nThis issue was originally reported in MASSEMBLY-842, but it seems the root cause in inside Commons Compress.\n\nConsider the attached {{invalid-entry.jar}}, whose contents, as shown by the {{zipinfo}} utility, is:\n\n{noformat}\n?rwsrwsrwt  2.0 unx        0 b- stor 17-Jan-15 16:06 META-INF/maven/\ndrwxr-xr-x  2.0 unx        0 b- stor 17-Jan-15 16:06 META-INF/\n{noformat}\n\nThere are some JAR files created by the Maven Assembly Plugin with content similar to this, and the entry {{META-INF/maven/}} has permissions 177777 (octal). Constructing a {{ZipFile}} from this file, the method {{isUnixSymlink}} incorrectly returns {{true}} for the entry {{META-INF/maven/}} (and it correctly returns {{false}} for the entry {{META-INF/}}.\n\nHere is a sample Java code that can be used to see the behaviour:\n\n{code:java}\npublic static void main(String[] args) throws IOException {\n    try (ZipFile zipFile = new ZipFile(new File(\"invalid-entry.jar\"))) {\n        printAttributes(zipFile, \"META-INF/\");\n        printAttributes(zipFile, \"META-INF/maven/\");\n    }\n}\n\nprivate static void printAttributes(ZipFile zipFile, String name) {\n    ZipArchiveEntry entry = zipFile.getEntriesInPhysicalOrder(name).iterator().next();\n    System.out.printf(\"%-17s: symlink:%-5s - unixMode:%s%n\", name, entry.isUnixSymlink(), entry.getUnixMode());\n}\n{code}\n\nThis code outputs:\n\n{noformat}\nMETA-INF/        : symlink:false - unixMode:16877\nMETA-INF/maven/  : symlink:true  - unixMode:65535\n{noformat}\n\nThe {{?rwsrwsrwt}} permissions show that the Zip entry is broken in the first place, but I think {{isUnixSymlink}} should still return {{false}} in that case, and not consider this entry to be a symlink.\n\nIt seems the fix would be to update {{isUnixSymlink}} and check whether the unix mode is equal to {{SHORT_MASK}}, and return {{false}} in that case as it would indicate a broken entry. This change does not break any existing tests, but I'm not sure if this is the proper fix.\n\n{code:java}\npublic boolean isUnixSymlink() {\n    int unixMode = getUnixMode();\n    return unixMode == SHORT_MASK ? false : (unixMode & UnixStat.LINK_FLAG) == UnixStat.LINK_FLAG;\n}\n{code}\n",
    "desc_source": "jira"
  },
  "Compress_43": {
    "description": "[Zip] Local `Version Needed To Extract` does not match Central Directory\nHi,\n\nThis is followup on an issue reported on Plexus Archiver - https://github.com/codehaus-plexus/plexus-archiver/issues/57\n\nPlexus Archiver uses {{ZipArchiveOutputStream}} to create zip archives. It constructs the {{ZipArchiveOutputStream}} using {{BufferedOutputStream}}. As a result the output do not provide random access and additional data descriptor records are added. Unfortunately this leads to different values being set for {{version needed to extract}} field in the local file header and in the central directory. It looks like that the root cause is the way the local header {{version needed to extract}} field value is calculated:\n{code:java}\n        if (phased &&  !isZip64Required(entry.entry, zip64Mode)){\n            putShort(INITIAL_VERSION, buf, LFH_VERSION_NEEDED_OFFSET);\n        } else {\n            putShort(versionNeededToExtract(zipMethod, hasZip64Extra(ze)), buf, LFH_VERSION_NEEDED_OFFSET);\n        }\n{code}\n\nAs you can see the need for data descriptors is not taken into account. On other hand when the central directory is created the following is used to determine the minimum required version\n\n{code:java}\n    private int versionNeededToExtract(final int zipMethod, final boolean zip64) {\n        if (zip64) {\n            return ZIP64_MIN_VERSION;\n        }\n        // requires version 2 as we are going to store length info\n        // in the data descriptor\n        return (isDeflatedToOutputStream(zipMethod)) ?\n                DATA_DESCRIPTOR_MIN_VERSION :\n                INITIAL_VERSION;\n    }\n{code}\n\nAs a side note: I'm not a zip expert by any means so I could be wrong, but my understanding is that if Deflate compression is used then the minimum required version should be 2.0 regardless if data descriptors are used or not.",
    "desc_source": "jira"
  },
  "Compress_44": {
    "description": "NullPointerException defect in ChecksumCalculatingInputStream#getValue()\nNullPointerException defect in ChecksumCalculatingInputStream#getValue() detected as stated in pull request 33: https://github.com/apache/commons-compress/pull/33\n\nFurthermore the following test describes the problem:\n\n{code:java}\n    @Test(expected = NullPointerException.class) //I assume this behaviour to be a bug or at least a defect.\n    public void testGetValueThrowsNullPointerException() {\n\n        ChecksumCalculatingInputStream checksumCalculatingInputStream = new ChecksumCalculatingInputStream(null,null);\n\n        checksumCalculatingInputStream.getValue();\n\n\n    }\n{code}\n\n",
    "desc_source": "jira"
  },
  "Compress_45": {
    "description": "TarUtils.formatLongOctalOrBinaryBytes never uses result of formatLongBinary\nif the length < 9, formatLongBinary is executed, then overwritten by the results of formatBigIntegerBinary. \n\nIf the results are not ignored, a unit test would fail.\n\nAlso, do the binary hacks  need to support negative numbers? ",
    "desc_source": "jira"
  },
  "Compress_46": {
    "description": "Tests failing under jdk 9 : one reflection issue, one change to ZipEntry related issue\nX5455_ExtendedTimestampTest is failing under JDK 9 , due to what appears to be a bogus value returned from getTime().  It seems like the test failure might be due to the changes introduced for this: \nhttps://bugs.openjdk.java.net/browse/JDK-8073497\n\nTests were run using intelliJ TestRunner, using the openjdk9 build from the tip of the jdk9 tree (not dev).  I believe that this is at most one commit away from what will be the RC (which was delayed at the last minute due to two issues, one of which was javadoc related, and the other hotspot. \n",
    "desc_source": "jira"
  },
  "Compress_47": {
    "description": "ZipArchiveInputStream#getNextZipEntry should verify compressed size is known for bzip2, implode etc.\n{code}\r\n        if (current.entry.getCompressedSize() != ArchiveEntry.SIZE_UNKNOWN) {\r\n            if (current.entry.getMethod() == ZipMethod.UNSHRINKING.getCode()) {\r\n                current.in = new UnshrinkingInputStream(new BoundedInputStream(in, current.entry.getCompressedSize()));\r\n            } else if (current.entry.getMethod() == ZipMethod.IMPLODING.getCode()) {\r\n                current.in = new ExplodingInputStream(\r\n                        current.entry.getGeneralPurposeBit().getSlidingDictionarySize(),\r\n                        current.entry.getGeneralPurposeBit().getNumberOfShannonFanoTrees(),\r\n                        new BoundedInputStream(in, current.entry.getCompressedSize()));\r\n            } else if (current.entry.getMethod() == ZipMethod.BZIP2.getCode()) {\r\n                current.in = new BZip2CompressorInputStream(new BoundedInputStream(in, current.entry.getCompressedSize()));\r\n            }\r\n        }\r\n{code}\r\n\r\nnever sets {{current.in}} if the compressed size is unknown which probably leads to a NullPointerException in {{read}} later. We should fail early with a useful error message instead.",
    "desc_source": "jira"
  }
}