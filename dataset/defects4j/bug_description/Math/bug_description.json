{
  "Math_1": {
    "description": "Fraction specified with maxDenominator and a value very close to a simple fraction should not throw an overflow exception\nAn overflow exception is thrown when a Fraction is initialized with a maxDenominator from a double that is very close to a simple\nfraction.  For example:\n\ndouble d = 0.5000000001;\nFraction f = new Fraction(d, 10);\n\nPatch with unit test on way.",
    "desc_source": "jira"
  },
  "Math_2": {
    "description": "HypergeometricDistribution.sample suffers from integer overflow\nHi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values -- the example code below should return a sample between 0 and 50, but usually returns -50.\n\n{code}\nimport org.apache.commons.math3.distribution.HypergeometricDistribution;\n\npublic class Foo {\n  public static void main(String[] args) {\n    HypergeometricDistribution a = new HypergeometricDistribution(\n        43130568, 42976365, 50);\n    System.out.printf(\"%d %d%n\", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints \"0 50\"\n    System.out.printf(\"%d%n\",a.sample());                                             // Prints \"-50\"\n  }\n}\n{code}\n\nIn the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean() -- instead of doing\n{code}\nreturn (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();\n{code}\nit could do:\n{code}\nreturn getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());\n{code}\nThis seemed to fix it, based on a quick test.",
    "desc_source": "jira"
  },
  "Math_3": {
    "description": "ArrayIndexOutOfBoundsException in MathArrays.linearCombination\nWhen MathArrays.linearCombination is passed arguments with length 1, it throws an ArrayOutOfBoundsException. This is caused by this line:\n\ndouble prodHighNext = prodHigh[1];\n\nlinearCombination should check the length of the arguments and fall back to simple multiplication if length == 1.",
    "desc_source": "jira"
  },
  "Math_4": {
    "description": "NPE when calling SubLine.intersection() with non-intersecting lines\nWhen calling SubLine.intersection() with two lines that not intersect, then a NullPointerException is thrown in Line.toSubSpace(). This bug is in the twod and threed implementations.\n\nThe attached patch fixes both implementations and adds the required test cases.\n\n",
    "desc_source": "jira"
  },
  "Math_5": {
    "description": "Complex.ZERO.reciprocal() returns NaN but should return INF.\nComplex.ZERO.reciprocal() returns NaN but should return INF.\n\nClass: org.apache.commons.math3.complex.Complex;\nMethod: reciprocal()\n@version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $\n",
    "desc_source": "jira"
  },
  "Math_6": {
    "description": "LevenbergMarquardtOptimizer reports 0 iterations\nThe method LevenbergMarquardtOptimizer.getIterations() does not report the correct number of iterations; It always returns 0. A quick look at the code shows that only SimplexOptimizer calls BaseOptimizer.incrementEvaluationsCount()\n\nI've put a test case below. Notice how the evaluations count is correctly incremented, but the iterations count is not.\n\n{noformat}\n    @Test\n    public void testGetIterations() {\n        // setup\n        LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();\n\n        // action\n        otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),\n                new Weight(new double[] { 1 }), new InitialGuess(\n                        new double[] { 3 }), new ModelFunction(\n                        new MultivariateVectorFunction() {\n                            @Override\n                            public double[] value(double[] point)\n                                    throws IllegalArgumentException {\n                                return new double[] { FastMath.pow(point[0], 4) };\n                            }\n                        }), new ModelFunctionJacobian(\n                        new MultivariateMatrixFunction() {\n                            @Override\n                            public double[][] value(double[] point)\n                                    throws IllegalArgumentException {\n                                return new double[][] { { 0.25 * FastMath.pow(\n                                        point[0], 3) } };\n                            }\n                        }));\n\n        // verify\n        assertThat(otim.getEvaluations(), greaterThan(1));\n        assertThat(otim.getIterations(), greaterThan(1));\n    }\n\n{noformat}",
    "desc_source": "jira"
  },
  "Math_7": {
    "description": "event state not updated if an unrelated event triggers a RESET_STATE during ODE integration\nWhen an ODE solver manages several different event types, there are some unwanted side effects.\n\nIf one event handler asks for a RESET_STATE (for integration state) when its eventOccurred method is called, the other event handlers that did not trigger an event in the same step are not updated correctly, due to an early return.\n\nAs a result, when the next step is processed with a reset integration state, the forgotten event still refer to the start date of the previous state. This implies that when these event handlers will be checked for In some cases, the function defining an event g(double t, double[] y) is called with state parameters y that are completely wrong. In one case when the y array should have contained values between -1 and +1, one function call got values up to 1.0e20.\n\nThe attached file reproduces the problem.\n",
    "desc_source": "jira"
  },
  "Math_8": {
    "description": "DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type\nCreating an array with {{Array.newInstance(singletons.get(0).getClass(), sampleSize)}} in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:\n* {{singleons.get(0)}} is of type T1, an sub-class of T, and\n* {{DiscreteDistribution.sample()}} returns an object which is of type T, but not of type T1.\n\nTo reproduce:\n{code}\nList<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>();\nlist.add(new Pair<Object, Double>(new Object() {}, new Double(0)));\nlist.add(new Pair<Object, Double>(new Object() {}, new Double(1)));\nnew DiscreteDistribution<Object>(list).sample(1);\n{code}\n\nAttaching a patch.",
    "desc_source": "jira"
  },
  "Math_9": {
    "description": "Line.revert() is imprecise\nLine.revert() only maintains ~10 digits for the direction. This becomes an issue when the line's position is evaluated far from the origin. A simple fix would be to use Vector3D.negate() for the direction.\n\nAlso, is there a reason why Line is not immutable? It is just comprised of two vectors.",
    "desc_source": "jira"
  },
  "Math_10": {
    "description": "DerivativeStructure.atan2(y,x) does not handle special cases properly\nThe four special cases +/-0 for both x and y should give the same values as Math.atan2 and FastMath.atan2. However, they give NaN for the value in all cases.",
    "desc_source": "jira"
  },
  "Math_11": {
    "description": "MultivariateNormalDistribution.density(double[]) returns wrong value when the dimension is odd\nTo reproduce:\n{code}\nAssert.assertEquals(0.398942280401433, new MultivariateNormalDistribution(new double[]{0}, new double[][]{{1}}).density(new double[]{0}), 1e-15);\n{code}",
    "desc_source": "jira"
  },
  "Math_12": {
    "description": "GammaDistribution cloning broken\nSerializing a GammaDistribution and deserializing it, does not result in a cloned distribution that produces the same samples.\n\nCause: GammaDistribution inherits from AbstractRealDistribution, which implements Serializable. AbstractRealDistribution has random, in which we have a Well19937c instance, which inherits from AbstractWell. AbstractWell implements Serializable. AbstractWell inherits from BitsStreamGenerator, which is not Serializable, but does have a private field 'nextGaussian'.\n\nSolution: Make BitStreamGenerator implement Serializable as well.\n\nThis probably affects other distributions as well.",
    "desc_source": "jira"
  },
  "Math_13": {
    "description": "new multivariate vector optimizers cannot be used with large number of weights\nWhen using the Weigth class to pass a large number of weights to multivariate vector optimizers, an nxn full matrix is created (and copied) when a n elements vector is used. This exhausts memory when n is large.\n\nThis happens for example when using curve fitters (even simple curve fitters like polynomial ones for low degree) with large number of points. I encountered this with curve fitting on 41200 points, which created a matrix with 1.7 billion elements.",
    "desc_source": "jira"
  },
  "Math_14": {
    "description": "new multivariate vector optimizers cannot be used with large number of weights\nWhen using the Weigth class to pass a large number of weights to multivariate vector optimizers, an nxn full matrix is created (and copied) when a n elements vector is used. This exhausts memory when n is large.\n\nThis happens for example when using curve fitters (even simple curve fitters like polynomial ones for low degree) with large number of points. I encountered this with curve fitting on 41200 points, which created a matrix with 1.7 billion elements.",
    "desc_source": "jira"
  },
  "Math_15": {
    "description": "FastMath.pow deviates from Math.pow for negative, finite base values with an exponent 2^52 < y < 2^53 \nAs reported by Jeff Hain:\n\npow(double,double):\nMath.pow(-1.0,5.000000000000001E15) = -1.0\nFastMath.pow(-1.0,5.000000000000001E15) = 1.0\n===> This is due to considering that power is an even\ninteger if it is >= 2^52, while you need to test\nthat it is >= 2^53 for it.\n===> replace\n\"if (y >= TWO_POWER_52 || y <= -TWO_POWER_52)\"\nwith\n\"if (y >= 2*TWO_POWER_52 || y <= -2*TWO_POWER_52)\"\nand that solves it.",
    "desc_source": "jira"
  },
  "Math_16": {
    "description": "FastMath.[cosh, sinh] do not support the same range of values as the Math counterparts\nAs reported by Jeff Hain:\n\ncosh(double) and sinh(double):\nMath.cosh(709.783) = 8.991046692770538E307\nFastMath.cosh(709.783) = Infinity\nMath.sinh(709.783) = 8.991046692770538E307\nFastMath.sinh(709.783) = Infinity\n===> This is due to using exp( x )/2 for values of |x|\nabove 20: the result sometimes should not overflow,\nbut exp( x ) does, so we end up with some infinity.\n===> for values of |x| >= StrictMath.log(Double.MAX_VALUE),\nexp will overflow, so you need to use that instead:\nfor x positive:\ndouble t = exp(x*0.5);\nreturn (0.5*t)*t;\nfor x negative:\ndouble t = exp(-x*0.5);\nreturn (-0.5*t)*t;",
    "desc_source": "jira"
  },
  "Math_17": {
    "description": "Dfp Dfp.multiply(int x) does not comply with the general contract FieldElement.multiply(int n)\nIn class {{org.apache.commons.math3.Dfp}},  the method {{multiply(int n)}} is limited to {{0 <= n <= 9999}}. This is not consistent with the general contract of {{FieldElement.multiply(int n)}}, where there should be no limitation on the values of {{n}}.",
    "desc_source": "jira"
  },
  "Math_18": {
    "description": "CMAESOptimizer with bounds fits finely near lower bound and coarsely near upper bound. \nWhen fitting with bounds, the CMAESOptimizer fits finely near the lower bound and coarsely near the upper bound.  This is because it internally maps the fitted parameter range into the interval [0,1].  The unit of least precision (ulp) between floating point numbers is much smaller near zero than near one.  Thus, fits have much better resolution near the lower bound (which is mapped to zero) than the upper bound (which is mapped to one).  I will attach a example program to demonstrate.",
    "desc_source": "jira"
  },
  "Math_19": {
    "description": "Wide bounds to CMAESOptimizer result in NaN parameters passed to fitness function\nIf you give large values as lower/upper bounds (for example -Double.MAX_VALUE as a lower bound), the optimizer can call the fitness function with parameters set to NaN.  My guess is this is due to FitnessFunction.encode/decode generating NaN when normalizing/denormalizing parameters.  For example, if the difference between the lower and upper bound is greater than Double.MAX_VALUE, encode could divide infinity by infinity.",
    "desc_source": "jira"
  },
  "Math_20": {
    "description": "CMAESOptimizer does not enforce bounds\nThe CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.",
    "desc_source": "jira"
  },
  "Math_21": {
    "description": "Correlated random vector generator fails (silently) when faced with zero rows in covariance matrix\nThe following three matrices (which are basically permutations of each other) produce different results when sampling a multi-variate Gaussian with the help of CorrelatedRandomVectorGenerator (sample covariances calculated in R, based on 10,000 samples):\n\nArray2DRowRealMatrix{\n{0.0,0.0,0.0,0.0,0.0},\n{0.0,0.013445532,0.01039469,0.009881156,0.010499559},\n{0.0,0.01039469,0.023006616,0.008196856,0.010732709},\n{0.0,0.009881156,0.008196856,0.019023866,0.009210099},\n{0.0,0.010499559,0.010732709,0.009210099,0.019107243}}\n\n> cov(data1)\n   V1 V2 V3 V4 V5\nV1 0 0.000000000 0.00000000 0.000000000 0.000000000\nV2 0 0.013383931 0.01034401 0.009913271 0.010506733\nV3 0 0.010344006 0.02309479 0.008374730 0.010759306\nV4 0 0.009913271 0.00837473 0.019005488 0.009187287\nV5 0 0.010506733 0.01075931 0.009187287 0.019021483\n\nArray2DRowRealMatrix{\n{0.013445532,0.01039469,0.0,0.009881156,0.010499559},\n{0.01039469,0.023006616,0.0,0.008196856,0.010732709},\n{0.0,0.0,0.0,0.0,0.0},\n{0.009881156,0.008196856,0.0,0.019023866,0.009210099},\n{0.010499559,0.010732709,0.0,0.009210099,0.019107243}}\n\n> cov(data2)\n            V1 V2 V3 V4 V5\nV1 0.006922905 0.010507692 0 0.005817399 0.010330529\nV2 0.010507692 0.023428918 0 0.008273152 0.010735568\nV3 0.000000000 0.000000000 0 0.000000000 0.000000000\nV4 0.005817399 0.008273152 0 0.004929843 0.009048759\nV5 0.010330529 0.010735568 0 0.009048759 0.018683544 \n\nArray2DRowRealMatrix{\n{0.013445532,0.01039469,0.009881156,0.010499559},\n{0.01039469,0.023006616,0.008196856,0.010732709},\n{0.009881156,0.008196856,0.019023866,0.009210099},\n{0.010499559,0.010732709,0.009210099,0.019107243}}\n\n> cov(data3)\n            V1          V2          V3          V4\nV1 0.013445047 0.010478862 0.009955904 0.010529542\nV2 0.010478862 0.022910522 0.008610113 0.011046353\nV3 0.009955904 0.008610113 0.019250975 0.009464442\nV4 0.010529542 0.011046353 0.009464442 0.019260317\n\n\nI've traced this back to the RectangularCholeskyDecomposition, which does not seem to handle the second matrix very well (decompositions in the same order as the matrices above):\n\nCorrelatedRandomVectorGenerator.getRootMatrix() = \nArray2DRowRealMatrix{{0.0,0.0,0.0,0.0,0.0},{0.0759577418122063,0.0876125188474239,0.0,0.0,0.0},{0.07764443622513505,0.05132821221460752,0.11976381821791235,0.0,0.0},{0.06662930527909404,0.05501661744114585,0.0016662506519307997,0.10749324207653632,0.0},{0.13822895138139477,0.0,0.0,0.0,0.0}}\nCorrelatedRandomVectorGenerator.getRank() = 5\n\nCorrelatedRandomVectorGenerator.getRootMatrix() = \nArray2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.0},{0.07764443622513505,0.13029949164628746,0.0},{0.0,0.0,0.0},{0.06662930527909404,0.023203936694855674,0.0},{0.13822895138139477,0.0,0.0}}\nCorrelatedRandomVectorGenerator.getRank() = 3\n\nCorrelatedRandomVectorGenerator.getRootMatrix() = \nArray2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.033913748226348225,0.07303890149947785},{0.07764443622513505,0.13029949164628746,0.0,0.0},{0.06662930527909404,0.023203936694855674,0.11851573313229945,0.0},{0.13822895138139477,0.0,0.0,0.0}}\nCorrelatedRandomVectorGenerator.getRank() = 4\n\nClearly, the rank of each of these matrices should be 4. The first matrix does not lead to incorrect results, but the second one does. Unfortunately, I don't know enough about the Cholesky decomposition to find the flaw in the implementation, and I could not find documentation for the \"rectangular\" variant (also not at the links provided in the javadoc).",
    "desc_source": "jira"
  },
  "Math_22": {
    "description": "Fix and then deprecate isSupportXxxInclusive in RealDistribution interface\nThe conclusion from [1] was never implemented. We should deprecate these\nproperties from the RealDistribution interface, but since removal\nwill have to wait until 4.0, we should agree on a precise\ndefinition and fix the code to match it in the mean time.\n\nThe definition that I propose is that isSupportXxxInclusive means\nthat when the density function is applied to the upper or lower\nbound of support returned by getSupportXxxBound, a finite (i.e. not\ninfinite), not NaN value is returned.\n\n[1] http://markmail.org/message/dxuxh7eybl7xejde\n",
    "desc_source": "jira"
  },
  "Math_23": {
    "description": "\"BrentOptimizer\" not always reporting the best point\n{{BrentOptimizer}} (package \"o.a.c.m.optimization.univariate\") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.",
    "desc_source": "jira"
  },
  "Math_24": {
    "description": "\"BrentOptimizer\" not always reporting the best point\n{{BrentOptimizer}} (package \"o.a.c.m.optimization.univariate\") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.",
    "desc_source": "jira"
  },
  "Math_25": {
    "description": "\"HarmonicFitter.ParameterGuesser\" sometimes fails to return sensible values\nThe inner class \"ParameterGuesser\" in \"HarmonicFitter\" (package \"o.a.c.m.optimization.fitting\") fails to compute a usable guess for the \"amplitude\" parameter.\n",
    "desc_source": "jira"
  },
  "Math_26": {
    "description": "Fraction(double, int) constructor strange behaviour\nThe Fraction constructor Fraction(double, int) takes a double value and a int maximal denominator, and approximates a fraction. When the double value is a large, negative number with many digits in the fractional part, and the maximal denominator is a big, positive integer (in the 100'000s), two distinct bugs can manifest:\n\n1: the constructor returns a positive Fraction. Calling Fraction(-33655.1677817278, 371880) returns the fraction 410517235/243036, which both has the wrong sign, and is far away from the absolute value of the given value\n\n2: the constructor does not manage to reduce the Fraction properly. Calling Fraction(-43979.60679604749, 366081) returns the fraction -1651878166/256677, which should have* been reduced to -24654898/3831.\n\nI have, as of yet, not found a solution. The constructor looks like this:\n\npublic Fraction(double value, int maxDenominator)\n        throws FractionConversionException\n    {\n       this(value, 0, maxDenominator, 100);\n    }\n\nIncreasing the 100 value (max iterations) does not fix the problem for all cases. Changing the 0-value (the epsilon, maximum allowed error) to something small does not work either, as this breaks the tests in FractionTest. \n\nThe problem is not neccissarily that the algorithm is unable to approximate a fraction correctly. A solution where a FractionConversionException had been thrown in each of these examples would probably be the best solution if an improvement on the approximation algorithm turns out to be hard to find.\n\nThis bug has been found when trying to explore the idea of axiom-based testing (http://bldl.ii.uib.no/testing.html). Attached is a java test class FractionTestByAxiom (junit, goes into org.apache.commons.math3.fraction) which shows these bugs through a simplified approach to this kind of testing, and a text file describing some of the value/maxDenominator combinations which causes one of these failures.\n\n* It is never specified in the documentation that the Fraction class guarantees that completely reduced rational numbers are constructed, but a comment inside the equals method claims that \"since fractions are always in lowest terms, numerators and can be compared directly for equality\", so it seems like this is the intention. ",
    "desc_source": "jira"
  },
  "Math_27": {
    "description": "Fraction percentageValue rare overflow\nThe percentageValue() method of the Fraction class works by first multiplying the Fraction by 100, then converting the Fraction to a double. This causes overflows when the numerator is greater than Integer.MAX_VALUE/100, even when the value of the fraction is far below this value.\n\nThe patch changes the method to first convert to a double value, and then multiply this value by 100 - the result should be the same, but with less overflows. An addition to the test for the method that covers this bug is also included.",
    "desc_source": "jira"
  },
  "Math_28": {
    "description": "Not expected UnboundedSolutionException\nSimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables.\n\nIn order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions.\nFirst iteration is runned with predefined set of input data with which the Solver gives back an appropriate result.\n\nThe problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values.\n\nWhat is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem.\n\nThe problem is formulated as\nmin(1*t + 0*L) (for every r-th subject)\ns.t.\n-q(r) + QL >= 0\nx(r)t - XL >= 0\nL >= 0\nwhere \nr = 1..R, \nL = {l(1), l(2), ..., l(R)} (vector of R rows and 1 column),\nQ - coefficients matrix MxR\nX - coefficients matrix NxR ",
    "desc_source": "jira"
  },
  "Math_29": {
    "description": "Bugs in RealVector.ebeMultiply(RealVector) and ebeDivide(RealVector)\n{{OpenMapRealVector.ebeMultiply(RealVector)}} and {{OpenMapRealVector.ebeDivide(RealVector)}} return wrong values when one entry of the specified {{RealVector}} is nan or infinity. The bug is easy to understand. Here is the current implementation of {{ebeMultiply}}\n\n{code:java}\n    public OpenMapRealVector ebeMultiply(RealVector v) {\n        checkVectorDimensions(v.getDimension());\n        OpenMapRealVector res = new OpenMapRealVector(this);\n        Iterator iter = entries.iterator();\n        while (iter.hasNext()) {\n            iter.advance();\n            res.setEntry(iter.key(), iter.value() * v.getEntry(iter.key()));\n        }\n        return res;\n    }\n{code}\n\nThe assumption is that for any double {{x}}, {{x * 0d == 0d}} holds, which is not true. The bug is easy enough to identify, but more complex to solve. The only solution I can come up with is to loop through *all* entries of v (instead of those entries which correspond to non-zero entries of this). I'm afraid about performance losses.\n\n",
    "desc_source": "jira"
  },
  "Math_30": {
    "description": "Mann-Whitney U Test Suffers From Integer Overflow With Large Data Sets\nWhen performing a Mann-Whitney U Test on large data sets (the attached test uses two 1500 element sets), intermediate integer values used in calculateAsymptoticPValue can overflow, leading to invalid results, such as p-values of NaN, or incorrect calculations.\n\nAttached is a patch, including a test, and a fix, which modifies the affected code to use doubles",
    "desc_source": "jira"
  },
  "Math_31": {
    "description": "inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.\nThe inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem.\n\n{{System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5));}}\n\nThis returns 499525, though it should be 499999.\n\nI'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.",
    "desc_source": "jira"
  },
  "Math_32": {
    "description": "BSPTree class and recovery of a Euclidean 3D BRep\nNew to the work here. Thanks for your efforts on this code.\n\nI create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(x,y,z) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem.\n\nAny ideas?\n",
    "desc_source": "jira"
  },
  "Math_33": {
    "description": "SimplexSolver gives bad results\nMethode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0\nin a simple test problem. It works well in commons-math-2.2. ",
    "desc_source": "jira"
  },
  "Math_34": {
    "description": "ListPopulation Iterator allows you to remove chromosomes from the population.\nCalling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.",
    "desc_source": "jira"
  },
  "Math_35": {
    "description": "Need range checks for elitismRate in ElitisticListPopulation constructors.\nThere is a range check for setting the elitismRate via ElitisticListPopulation's setElitismRate method, but not via the constructors.",
    "desc_source": "jira"
  },
  "Math_36": {
    "description": "BigFraction.doubleValue() returns Double.NaN for large numerators or denominators\nThe current implementation of doubleValue() divides numerator.doubleValue() / denominator.doubleValue().  BigInteger.doubleValue() fails for any number greater than Double.MAX_VALUE.  So if the user has 308-digit numerator or denominator, the resulting quotient fails, even in cases where the result would be well inside Double's range.\n\nI have a patch to fix it, if I can figure out how to attach it here I will.",
    "desc_source": "jira"
  },
  "Math_37": {
    "description": "[math] Complex Tanh for \"big\" numbers\nHi,\n\nIn Complex.java the tanh is computed with the following formula:\n\ntanh(a + bi) = sinh(2a)/(cosh(2a)+cos(2b)) + [sin(2b)/(cosh(2a)+cos(2b))]i\n\nThe problem that I'm finding is that as soon as \"a\" is a \"big\" number,\nboth sinh(2a) and cosh(2a) are infinity and then the method tanh returns in\nthe real part NaN (infinity/infinity) when it should return 1.0.\n\nWouldn't it be appropiate to add something as in the FastMath library??:\n\nif (real>20.0){\n      return createComplex(1.0, 0.0);\n}\nif (real<-20.0){\n      return createComplex(-1.0, 0.0);\n}\n\n\nBest regards,\n\nJBB\n",
    "desc_source": "jira"
  },
  "Math_38": {
    "description": "Errors in BOBYQAOptimizer when numberOfInterpolationPoints is greater than 2*dim+1\nI've been having trouble getting BOBYQA to minimize a function (actually a non-linear least squares fit) so as one change I increased the number of interpolation points.  It seems that anything larger than 2*dim+1 causes an error (typically at\n\nline 1662\n                   interpolationPoints.setEntry(nfm, ipt, interpolationPoints.getEntry(ipt, ipt));\n\nI'm guessing there is an off by one error in the translation from FORTRAN.  Changing the BOBYQAOptimizerTest as follows (increasing number of interpolation points by one) will cause failures.\n\nBruce\n\n\n\nIndex: src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java\n===================================================================\n--- src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java\t(revision 1221065)\n+++ src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java\t(working copy)\n@@ -258,7 +258,7 @@\n //        RealPointValuePair result = optim.optimize(100000, func, goal, startPoint);\n         final double[] lB = boundaries == null ? null : boundaries[0];\n         final double[] uB = boundaries == null ? null : boundaries[1];\n-        BOBYQAOptimizer optim = new BOBYQAOptimizer(2 * dim + 1);\n+        BOBYQAOptimizer optim = new BOBYQAOptimizer(2 * dim + 2);\n         RealPointValuePair result = optim.optimize(maxEvaluations, func, goal, startPoint, lB, uB);\n //        System.out.println(func.getClass().getName() + \" = \" \n //              + optim.getEvaluations() + \" f(\");\n",
    "desc_source": "jira"
  },
  "Math_39": {
    "description": "too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)\nAdaptive step size integrators compute the first step size by themselves if it is not provided.\nFor embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.",
    "desc_source": "jira"
  },
  "Math_40": {
    "description": "BracketingNthOrderBrentSolver exceeds maxIterationCount while updating always the same boundary\nIn some cases, the aging feature in BracketingNthOrderBrentSolver fails.\nIt attempts to balance the bracketing points by targeting a non-zero value instead of the real root. However, the chosen target is too close too zero, and the inverse polynomial approximation is always on the same side, thus always updates the same bracket.\nIn the real used case for a large program, I had a bracket point xA = 12500.0, yA = 3.7e-16, agingA = 0, which is the (really good) estimate of the zero on one side of the root and xB = 12500.03, yB = -7.0e-5, agingB = 97. This shows that the bracketing interval is completely unbalanced, and we never succeed to rebalance it as we always updates (xA, yA) and never updates (xB, yB).",
    "desc_source": "jira"
  },
  "Math_41": {
    "description": "One of Variance.evaluate() methods does not work correctly\nThe method org.apache.commons.math.stat.descriptive.moment.Variance.evaluate(double[] values, double[] weights, double mean, int begin, int length) does not work properly. Looks loke it ignores the length parameter and grabs the whole dataset.\nSimilar method in Mean class seems to work.\nI did not check other methods taking the part of the array; they may have the same problem.\n\nWorkaround: I had to shrink my arrays and use the method without the length.",
    "desc_source": "jira"
  },
  "Math_42": {
    "description": "Negative value with restrictNonNegative\nProblem: commons-math-2.2 SimplexSolver.\n\nA variable with 0 coefficient may be assigned a negative value nevertheless restrictToNonnegative flag in call:\nSimplexSolver.optimize(function, constraints, GoalType.MINIMIZE, true);\n\nFunction\n1 * x + 1 * y + 0\n\nConstraints:\n1 * x + 0 * y = 1\n\nResult:\nx = 1; y = -1;\n\nProbably variables with 0 coefficients are omitted at some point of computation and because of that the restrictions do not affect their values.",
    "desc_source": "jira"
  },
  "Math_43": {
    "description": "Statistics.setVarianceImpl makes getStandardDeviation produce NaN\nInvoking SummaryStatistics.setVarianceImpl(new Variance(true/false) makes getStandardDeviation produce NaN. The code to reproduce it:\n\n{code:java}\nint[] scores = {1, 2, 3, 4};\nSummaryStatistics stats = new SummaryStatistics();\nstats.setVarianceImpl(new Variance(false)); //use \"population variance\"\nfor(int i : scores) {\n  stats.addValue(i);\n}\ndouble sd = stats.getStandardDeviation();\nSystem.out.println(sd);\n{code}\n\nA workaround suggested by Mikkel is:\n{code:java}\n  double sd = FastMath.sqrt(stats.getSecondMoment() / stats.getN());\n{code}",
    "desc_source": "jira"
  },
  "Math_44": {
    "description": "Incomplete reinitialization with some events handling\nI get a bug with event handling: I track 2 events that occur in the same step, when the first one is accepted, it resets the state but the reinitialization is not complete and the second one becomes unable to find its way.\nI can't give my context, which is rather large, but I tried a patch that works for me, unfortunately it breaks the unit tests.",
    "desc_source": "jira"
  },
  "Math_45": {
    "description": "Integer overflow in OpenMapRealMatrix\ncomputeKey() has an integer overflow. Since it is a sparse matrix, this is quite easily encountered long before heap space is exhausted. The attached code demonstrates the problem, which could potentially be a security vulnerability (for example, if one was to use this matrix to store access control information).\n\nWorkaround: never create an OpenMapRealMatrix with more cells than are addressable with an int.",
    "desc_source": "jira"
  },
  "Math_46": {
    "description": "Division by zero\nIn class {{Complex}}, division by zero always returns NaN. I think that it should return NaN only when the numerator is also {{ZERO}}, otherwise the result should be {{INF}}. See [here|http://en.wikipedia.org/wiki/Riemann_sphere#Arithmetic_operations].\n",
    "desc_source": "jira"
  },
  "Math_47": {
    "description": "Division by zero\nIn class {{Complex}}, division by zero always returns NaN. I think that it should return NaN only when the numerator is also {{ZERO}}, otherwise the result should be {{INF}}. See [here|http://en.wikipedia.org/wiki/Riemann_sphere#Arithmetic_operations].\n",
    "desc_source": "jira"
  },
  "Math_48": {
    "description": "\"RegulaFalsiSolver\" failure\nThe following unit test:\n{code}\n@Test\npublic void testBug() {\n    final UnivariateRealFunction f = new UnivariateRealFunction() {\n            @Override\n            public double value(double x) {\n                return Math.exp(x) - Math.pow(Math.PI, 3.0);\n            }\n        };\n\n    UnivariateRealSolver solver = new RegulaFalsiSolver();\n    double root = solver.solve(100, f, 1, 10);\n}\n{code}\nfails with\n{noformat}\nillegal state: maximal count (100) exceeded: evaluations\n{noformat}\n\nUsing \"PegasusSolver\", the answer is found after 17 evaluations.\n",
    "desc_source": "jira"
  },
  "Math_49": {
    "description": "MathRuntimeException with simple ebeMultiply on OpenMapRealVector\nThe following piece of code\n{code:java}\nimport org.apache.commons.math.linear.OpenMapRealVector;\nimport org.apache.commons.math.linear.RealVector;\n\npublic class DemoBugOpenMapRealVector {\n    public static void main(String[] args) {\n        final RealVector u = new OpenMapRealVector(3, 1E-6);\n        u.setEntry(0, 1.);\n        u.setEntry(1, 0.);\n        u.setEntry(2, 2.);\n        final RealVector v = new OpenMapRealVector(3, 1E-6);\n        v.setEntry(0, 0.);\n        v.setEntry(1, 3.);\n        v.setEntry(2, 0.);\n        System.out.println(u);\n        System.out.println(v);\n        System.out.println(u.ebeMultiply(v));\n    }\n}\n{code}\nraises an exception\n{noformat}\norg.apache.commons.math.linear.OpenMapRealVector@7170a9b6\nException in thread \"main\" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating\n\tat org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373)\n\tat org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564)\n\tat org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372)\n\tat org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1)\n\tat DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)\n{noformat}\n",
    "desc_source": "jira"
  },
  "Math_50": {
    "description": "\"RegulaFalsiSolver\" failure\nThe following unit test:\n{code}\n@Test\npublic void testBug() {\n    final UnivariateRealFunction f = new UnivariateRealFunction() {\n            @Override\n            public double value(double x) {\n                return Math.exp(x) - Math.pow(Math.PI, 3.0);\n            }\n        };\n\n    UnivariateRealSolver solver = new RegulaFalsiSolver();\n    double root = solver.solve(100, f, 1, 10);\n}\n{code}\nfails with\n{noformat}\nillegal state: maximal count (100) exceeded: evaluations\n{noformat}\n\nUsing \"PegasusSolver\", the answer is found after 17 evaluations.\n",
    "desc_source": "jira"
  },
  "Math_51": {
    "description": "\"RegulaFalsiSolver\" failure\nThe following unit test:\n{code}\n@Test\npublic void testBug() {\n    final UnivariateRealFunction f = new UnivariateRealFunction() {\n            @Override\n            public double value(double x) {\n                return Math.exp(x) - Math.pow(Math.PI, 3.0);\n            }\n        };\n\n    UnivariateRealSolver solver = new RegulaFalsiSolver();\n    double root = solver.solve(100, f, 1, 10);\n}\n{code}\nfails with\n{noformat}\nillegal state: maximal count (100) exceeded: evaluations\n{noformat}\n\nUsing \"PegasusSolver\", the answer is found after 17 evaluations.\n",
    "desc_source": "jira"
  },
  "Math_52": {
    "description": "numerical problems in rotation creation\nbuilding a rotation from the following vector pairs leads to NaN:\nu1 = -4921140.837095533, -2.1512094250440013E7, -890093.279426377\nu2 = -2.7238580938724895E9, -2.169664921341876E9, 6.749688708885301E10\nv1 = 1, 0, 0\nv2 = 0, 0, 1\n\nThe constructor first changes the (v1, v2) pair into (v1', v2') ensuring the following scalar products hold:\n <v1'|v1'> == <u1|u1>\n <v2'|v2'> == <u2|u2>\n <u1 |u2>  == <v1'|v2'>\n\nOnce the (v1', v2') pair has been computed, we compute the cross product:\n  k = (v1' - u1)^(v2' - u2)\n\nand the scalar product:\n  c = <k | (u1^u2)>\n\nBy construction, c is positive or null and the quaternion axis we want to build is q = k/[2*sqrt(c)].\nc should be null only if some of the vectors are aligned, and this is dealt with later in the algorithm.\n\nHowever, there are numerical problems with the vector above with the way these computations are done, as shown\nby the following comparisons, showing the result we get from our Java code and the result we get from manual\ncomputation with the same formulas but with enhanced precision:\n\ncommons math:   k = 38514476.5,            -84.,                           -1168590144\nhigh precision: k = 38514410.36093388...,  -0.374075245201180409222711..., -1168590152.10599715208...\n\nand it becomes worse when computing c because the vectors are almost orthogonal to each other, hence inducing additional cancellations. We get:\ncommons math    c = -1.2397173627587605E20\nhigh precision: c =  558382746168463196.7079627...\n\nWe have lost ALL significant digits in cancellations, and even the sign is wrong!\n",
    "desc_source": "jira"
  },
  "Math_53": {
    "description": "Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same\nFor both Complex add and subtract, the javadoc states that\n\n{code}\n     * If either this or <code>rhs</code> has a NaN value in either part,\n     * {@link #NaN} is returned; otherwise Inifinite and NaN values are\n     * returned in the parts of the result according to the rules for\n     * {@link java.lang.Double} arithmetic\n{code}\n\nSubtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).\n",
    "desc_source": "jira"
  },
  "Math_54": {
    "description": "class Dfp toDouble method return -inf whan Dfp value is 0 \"zero\"\nI found a bug in the toDouble() method of the Dfp class.\nIf the Dfp's value is 0 \"zero\", the toDouble() method returns a  negative infini.\n\nThis is because the double value returned has an exposant equal to 0xFFF \nand a significand is equal to 0.\nIn the IEEE754 this is a -inf.\n\nTo be equal to zero, the exposant and the significand must be equal to zero.\n\nA simple test case is :\n----------------------------------------------\nimport org.apache.commons.math.dfp.DfpField;\n\n\npublic class test {\n\n\t/**\n\t * @param args\n\t */\n\tpublic static void main(String[] args) {\n\t\tDfpField field = new DfpField(100);\n\t\tSystem.out.println(\"toDouble value of getZero() =\"+field.getZero().toDouble()+\n\t\t\t\t\"\\ntoDouble value of newDfp(0.0) =\"+\n\t\t\t\tfield.newDfp(0.0).toDouble());\n\t}\n}\n\nMay be the simplest way to fix it is to test the zero equality at the begin of the toDouble() method, to be able to return the correctly signed zero ?\n",
    "desc_source": "jira"
  },
  "Math_55": {
    "description": "Vector3D.crossProduct is sensitive to numerical cancellation\nCross product implementation uses the naive formulas (y1 z2 - y2 z1, ...). These formulas fail when vectors are almost colinear, like in the following example:\n{code}\nVector3D v1 = new Vector3D(9070467121.0, 4535233560.0, 1);\nVector3D v2 = new Vector3D(9070467123.0, 4535233561.0, 1);\nSystem.out.println(Vector3D.crossProduct(v1, v2));\n{code}\n\nThe previous code displays { -1, 2, 0 } instead of the correct answer { -1, 2, 1 }",
    "desc_source": "jira"
  },
  "Math_56": {
    "description": "MultidimensionalCounter.getCounts(int) returns wrong array of indices\nMultidimensionalCounter counter = new MultidimensionalCounter(2, 4);\nfor (Integer i : counter) {\n    int[] x = counter.getCounts(i);\n    System.out.println(i + \" \" + Arrays.toString(x));\n}\n\nOutput is:\n0 [0, 0]\n1 [0, 1]\n2 [0, 2]\n3 [0, 2]   <=== should be [0, 3]\n4 [1, 0]\n5 [1, 1]\n6 [1, 2]\n7 [1, 2]   <=== should be [1, 3]",
    "desc_source": "jira"
  },
  "Math_57": {
    "description": "Truncation issue in KMeansPlusPlusClusterer\nThe for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable\n  int sum = 0;\nThis variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1.\n\nAs an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.",
    "desc_source": "jira"
  },
  "Math_58": {
    "description": "GaussianFitter Unexpectedly Throws NotStrictlyPositiveException\nRunning the following:\n\n    \tdouble[] observations = \n    \t{ \n    \t\t\t1.1143831578403364E-29, \n    \t\t\t 4.95281403484594E-28, \n    \t\t\t 1.1171347211930288E-26, \n    \t\t\t 1.7044813962636277E-25, \n    \t\t\t 1.9784716574832164E-24, \n    \t\t\t 1.8630236407866774E-23, \n    \t\t\t 1.4820532905097742E-22, \n    \t\t\t 1.0241963854632831E-21, \n    \t\t\t 6.275077366673128E-21, \n    \t\t\t 3.461808994532493E-20, \n    \t\t\t 1.7407124684715706E-19, \n    \t\t\t 8.056687953553974E-19, \n    \t\t\t 3.460193945992071E-18, \n    \t\t\t 1.3883326374011525E-17, \n    \t\t\t 5.233894983671116E-17, \n    \t\t\t 1.8630791465263745E-16, \n    \t\t\t 6.288759227922111E-16, \n    \t\t\t 2.0204433920597856E-15, \n    \t\t\t 6.198768938576155E-15, \n    \t\t\t 1.821419346860626E-14, \n    \t\t\t 5.139176445538471E-14, \n    \t\t\t 1.3956427429045787E-13, \n    \t\t\t 3.655705706448139E-13, \n    \t\t\t 9.253753324779779E-13, \n    \t\t\t 2.267636001476696E-12, \n    \t\t\t 5.3880460095836855E-12, \n    \t\t\t 1.2431632654852931E-11 \n    \t};\n  \n    \tGaussianFitter g = \n    \t\tnew GaussianFitter(new LevenbergMarquardtOptimizer());\n    \t\n    \tfor (int index = 0; index < 27; index++)\n    \t{\n    \t\tg.addObservedPoint(index, observations[index]);\n    \t}\n       \tg.fit();\n\nResults in:\n\norg.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0)\n\tat org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184)\n\tat org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129)\n\n\nI'm guessing the initial guess for sigma is off.  ",
    "desc_source": "jira"
  },
  "Math_59": {
    "description": "FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f\nFastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f.\n\nThis is because the wrong variable is returned.\n\nThe bug was not detected by the test case \"testMinMaxFloat()\" because that has a bug too - it tests doubles, not floats.",
    "desc_source": "jira"
  },
  "Math_60": {
    "description": "ConvergenceException in NormalDistributionImpl.cumulativeProbability()\nI get a ConvergenceException in  NormalDistributionImpl.cumulativeProbability() for very large/small parameters including Infinity, -Infinity.\nFor instance in the following code:\n\n\t@Test\n\tpublic void testCumulative() {\n\t\tfinal NormalDistribution nd = new NormalDistributionImpl();\n\t\tfor (int i = 0; i < 500; i++) {\n\t\t\tfinal double val = Math.exp(i);\n\t\t\ttry {\n\t\t\t\tSystem.out.println(\"val = \" + val + \" cumulative = \" + nd.cumulativeProbability(val));\n\t\t\t} catch (MathException e) {\n\t\t\t\te.printStackTrace();\n\t\t\t\tfail();\n\t\t\t}\n\t\t}\n\t}\n\nIn version 2.0, I get no exception. \n\nMy suggestion is to change in the implementation of cumulativeProbability(double) to catch all ConvergenceException (and return for very large and very small values), not just MaxIterationsExceededException.\n",
    "desc_source": "jira"
  },
  "Math_61": {
    "description": "Dangerous code in \"PoissonDistributionImpl\"\nIn the following excerpt from class \"PoissonDistributionImpl\":\n\n{code:title=PoissonDistributionImpl.java|borderStyle=solid}\n    public PoissonDistributionImpl(double p, NormalDistribution z) {\n        super();\n        setNormal(z);\n        setMean(p);\n    }\n{code}\n\n(1) Overridable methods are called within the constructor.\n(2) The reference \"z\" is stored and modified within the class.\n\nI've encountered problem (1) in several classes while working on issue 348. In those cases, in order to remove potential problems, I copied/pasted the body of the \"setter\" methods inside the constructor but I think that a more elegant solution would be to remove the \"setters\" altogether (i.e. make the classes immutable).\nProblem (2) can also create unexpected behaviour. Is it really necessary to pass the \"NormalDistribution\" object; can't it be always created within the class?\n",
    "desc_source": "jira"
  },
  "Math_62": {
    "description": "Miscellaneous issues concerning the \"optimization\" package\nRevision 990792 contains changes triggered the following issues:\n* [MATH-394|https://issues.apache.org/jira/browse/MATH-394]\n* [MATH-397|https://issues.apache.org/jira/browse/MATH-397]\n* [MATH-404|https://issues.apache.org/jira/browse/MATH-404]\n\nThis issue collects the currently still unsatisfactory code (not necessarily sorted in order of annoyance):\n# \"BrentOptimizer\": a specific convergence checker must be used. \"LevenbergMarquardtOptimizer\" also has specific convergence checks.\n# Trying to make convergence checking independent of the optimization algorithm creates problems (conceptual and practical):\n ** See \"BrentOptimizer\" and \"LevenbergMarquardtOptimizer\", the algorithm passes \"points\" to the convergence checker, but the actual meaning of the points can very well be different in the caller (optimization algorithm) and the callee (convergence checker).\n ** In \"PowellOptimizer\" the line search (\"BrentOptimizer\") tolerances depend on the tolerances within the main algorithm. Since tolerances come with \"ConvergenceChecker\" and so can be changed at any time, it is awkward to adapt the values within the line search optimizer without exposing its internals (\"BrentOptimizer\" field) to the enclosing class (\"PowellOptimizer\").\n# Given the numerous changes, some Javadoc comments might be out-of-sync, although I did try to update them all.\n# Class \"DirectSearchOptimizer\" (in package \"optimization.direct\") inherits from class \"AbstractScalarOptimizer\" (in package \"optimization.general\").\n# Some interfaces are defined in package \"optimization\" but their base implementations (abstract class that contain the boiler-plate code) are in package \"optimization.general\" (e.g. \"DifferentiableMultivariateVectorialOptimizer\" and \"BaseAbstractVectorialOptimizer\").\n# No check is performed to ensure the the convergence checker has been set (see e.g. \"BrentOptimizer\" and \"PowellOptimizer\"); if it hasn't there will be a NPE. The alternative is to initialize a default checker that will never be used in case the user had intended to explicitly sets the checker.\n# \"NonLinearConjugateGradientOptimizer\": Ugly workaround for the checked \"ConvergenceException\".\n# Everywhere, we trail the checked \"FunctionEvaluationException\" although it is never used.\n# There remains some duplicate code (such as the \"multi-start loop\" in the various \"MultiStart...\" implementations).\n# The \"ConvergenceChecker\" interface is very general (the \"converged\" method can take any number of \"...PointValuePair\"). However there remains a \"semantic\" problem: One cannot be sure that the list of points means the same thing for the caller of \"converged\" and within the implementation of the \"ConvergenceChecker\" that was independently set.\n# It is not clear whether it is wise to aggregate the counter of gradient evaluations to the function evaluation counter. In \"LevenbergMarquartdOptimizer\" for example, it would be unfair to do so. Currently I had to remove all tests referring to gradient and Jacobian evaluations.\n# In \"AbstractLeastSquaresOptimizer\" and \"LevenbergMarquardtOptimizer\", occurences of \"OptimizationException\" were replaced by the unchecked \"ConvergenceException\" but in some cases it might not be the most appropriate one.\n# \"MultiStartUnivariateRealOptimizer\": in the other classes (\"MultiStartMultivariate...\") similar to this one, the randomization is on the firts-guess value while in this class, it is on the search interval. I think that here also we should randomly choose the start value (within the user-selected interval).\n# The Javadoc utility raises warnings (see output of \"mvn site\") which I couldn't figure out how to correct.\n# Some previously existing classes and interfaces have become no more than a specialisation of new \"generics\" classes; it might be interesting to remove them in order to reduce the number of classes and thus limit the potential for confusion.\n",
    "desc_source": "jira"
  },
  "Math_63": {
    "description": "NaN in \"equals\" methods\nIn \"MathUtils\", some \"equals\" methods will return true if both argument are NaN.\nUnless I'm mistaken, this contradicts the IEEE standard.\n\nIf nobody objects, I'm going to make the changes.\n",
    "desc_source": "jira"
  },
  "Math_64": {
    "description": "Inconsistent result from Levenberg-Marquardt\nLevenberg-Marquardt (its method doOptimize) returns a VectorialPointValuePair.  However, the class holds the optimum point, the vector of the objective function, the cost and residuals.  The value returns by doOptimize does not always corresponds to the point which leads to the residuals and cost",
    "desc_source": "jira"
  },
  "Math_65": {
    "description": "weight versus sigma in AbstractLeastSquares\nIn AbstractLeastSquares, residualsWeights contains the WEIGHTS assigned to each observation.  In the method getRMS(), these weights are multiplicative as they should. unlike in getChiSquare() where it appears at the denominator!   If the weight is really the weight of the observation, it should multiply the square of the residual even in the computation of the chi2.\n\n Once corrected, getRMS() can even reduce\n\n public double getRMS() {return Math.sqrt(getChiSquare()/rows);}",
    "desc_source": "jira"
  },
  "Math_66": {
    "description": "Bugs in \"BrentOptimizer\"\nI apologize for having provided a buggy implementation of Brent's optimization algorithm (class \"BrentOptimizer\" in package \"optimization.univariate\").\nThe unit tests didn't show that there was something wrong, although (from the \"changes.xml\" file) I discovered that, at the time, Luc had noticed something weird in the implementation's behaviour.\nComparing with an implementation in Python, I could figure out the fixes. I'll modify \"BrentOptimizer\" and add a test. I also propose to change the name of the unit test class from \"BrentMinimizerTest\" to \"BrentOptimizerTest\".\n",
    "desc_source": "jira"
  },
  "Math_67": {
    "description": "Method \"getResult()\" in \"MultiStartUnivariateRealOptimizer\"\nIn \"MultiStartUnivariateRealOptimizer\" (package \"optimization\"), the method \"getResult\" returns the result of the last run of the \"underlying\" optimizer; this last result might not be the best one, in which case it will not correspond to the value returned by the \"optimize\" method. This is confusing and does not seem very useful. I think that \"getResult\" should be defined as\n{code} \npublic double getResult() {\n    return optima[0];\n}\n{code}\nand similarly\n{code}\npublic double getFunctionValue() {\n    return optimaValues[0];\n}\n{code}\n",
    "desc_source": "jira"
  },
  "Math_68": {
    "description": "LevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it\nLevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it. This makes it hard to specify custom stopping criteria for the optimizer.",
    "desc_source": "jira"
  },
  "Math_69": {
    "description": "PearsonsCorrelation.getCorrelationPValues() precision limited by machine epsilon\nSimilar to the issue described in MATH-201, using PearsonsCorrelation.getCorrelationPValues() with many treatments results in p-values that are continuous down to 2.2e-16 but that drop to 0 after that.\n\nIn MATH-201, the problem was described as such:\n> So in essence, the p-value returned by TTestImpl.tTest() is:\n> \n> 1.0 - (cumulativeProbability(t) - cumulativeProbabily(-t))\n> \n> For large-ish t-statistics, cumulativeProbabilty(-t) can get quite small, and cumulativeProbabilty(t) can get very close to 1.0. When \n> cumulativeProbability(-t) is less than the machine epsilon, we get p-values equal to zero because:\n> \n> 1.0 - 1.0 + 0.0 = 0.0\n\nThe solution in MATH-201 was to modify the p-value calculation to this:\n> p = 2.0 * cumulativeProbability(-t)\n\nHere, the problem is similar.  From PearsonsCorrelation.getCorrelationPValues():\n  p = 2 * (1 - tDistribution.cumulativeProbability(t));\n\nDirectly calculating the p-value using identical code as PearsonsCorrelation.getCorrelationPValues(), but with the following change seems to solve the problem:\n  p = 2 * (tDistribution.cumulativeProbability(-t));\n\n\n\n\n",
    "desc_source": "jira"
  },
  "Math_70": {
    "description": "BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException\nMethod \n\n    BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)  \n\ninvokes \n\n    BisectionSolver.solve(double min, double max) \n\nwhich throws NullPointerException, as member variable\n\n    UnivariateRealSolverImpl.f \n\nis null.\n\nInstead the method:\n\n    BisectionSolver.solve(final UnivariateRealFunction f, double min, double max)\n\nshould be called.\n\nSteps to reproduce:\n\ninvoke:\n\n     new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5);\n\nNullPointerException will be thrown.\n\n\n",
    "desc_source": "jira"
  },
  "Math_71": {
    "description": "ODE integrator goes past specified end of integration range\nEnd of integration range in ODE solving is handled as an event.\nIn some cases, numerical accuracy in events detection leads to error in events location.\nThe following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.\n{code}\n  public void testMissedEvent() throws IntegratorException, DerivativeException {\n          final double t0 = 1878250320.0000029;\n          final double t =  1878250379.9999986;\n          FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {\n            \n            public int getDimension() {\n                return 1;\n            }\n            \n            public void computeDerivatives(double t, double[] y, double[] yDot)\n                throws DerivativeException {\n                yDot[0] = y[0] * 1.0e-6;\n            }\n        };\n\n        DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,\n                                                                               1.0e-10, 1.0e-10);\n\n        double[] y = { 1.0 };\n        integrator.setInitialStepSize(60.0);\n        double finalT = integrator.integrate(ode, t0, y, t, y);\n        Assert.assertEquals(t, finalT, 1.0e-6);\n    }\n\n{code}",
    "desc_source": "jira"
  },
  "Math_72": {
    "description": "Brent solver returns the wrong value if either bracket endpoint is root\nThe solve(final UnivariateRealFunction f, final double min, final double max, final double initial) function returns yMin or yMax if min or max are deemed to be roots, respectively, instead of min or max.",
    "desc_source": "jira"
  },
  "Math_73": {
    "description": "Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign\nJavadoc for \"public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)\" claims that \"if the values of the function at the three points have the same sign\" an IllegalArgumentException is thrown. This case isn't even checked.",
    "desc_source": "jira"
  },
  "Math_74": {
    "description": "Wrong parameter for first step size guess for Embedded Runge Kutta methods\nIn a space application using DOP853 i detected what seems to be a bad parameter in the call to the method  initializeStep of class AdaptiveStepsizeIntegrator.\n\nHere, DormandPrince853Integrator is a subclass for EmbeddedRungeKuttaIntegrator which perform the call to initializeStep at the beginning of its method integrate(...)\n\nThe problem comes from the array \"scale\" that is used as a parameter in the call off initializeStep(..)\n\nFollowing the theory described by Hairer in his book \"Solving Ordinary Differential Equations 1 : Nonstiff Problems\", the scaling should be :\n\nsci = Atol i + |y0i| * Rtoli\n\nWhereas EmbeddedRungeKuttaIntegrator uses :  sci = Atoli\n\nNote that the Gragg-Bulirsch-Stoer integrator uses the good implementation \"sci = Atol i + |y0i| * Rtoli  \" when he performs the call to the same method initializeStep(..)\n\nIn the method initializeStep, the error leads to a wrong step size h used to perform an  Euler step. Most of the time it is unvisible for the user.\nBut in my space application the Euler step with this wrong step size h (much bigger than it should be)  makes an exception occur (my satellite hits the ground...)\n\n\nTo fix the bug, one should use the same algorithm as in the rescale method in GraggBulirschStoerIntegrator\nFor exemple :\n\n final double[] scale= new double[y0.length];;\n          \n          if (vecAbsoluteTolerance == null) {\n              for (int i = 0; i < scale.length; ++i) {\n                final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));\n                scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * yi;\n              }\n            } else {\n              for (int i = 0; i < scale.length; ++i) {\n                final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));\n                scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * yi;\n              }\n            }\n          \n          hNew = initializeStep(equations, forward, getOrder(), scale,\n                           stepStart, y, yDotK[0], yTmp, yDotK[1]);\n\n\n\nSorry for the length of this message, looking forward to hearing from you soon\n\nVincent Morand\n\n\n\n\n",
    "desc_source": "jira"
  },
  "Math_75": {
    "description": "In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable) \nDrop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change\n\nFrequency.java\n\n   /**\n      * Returns the percentage of values that are equal to v\n     * @deprecated replaced by {@link #getPct(Comparable)} as of 2.0\n     */\n    @Deprecated\n    public double getPct(Object v) {\n        return getCumPct((Comparable<?>) v);\n    }",
    "desc_source": "jira"
  },
  "Math_76": {
    "description": "NaN singular value from SVD\nThe following jython code\nStart code\n\nfrom org.apache.commons.math.linear import *\n \nAlist = [[1.0, 2.0, 3.0],[2.0,3.0,4.0],[3.0,5.0,7.0]]\n \nA = Array2DRowRealMatrix(Alist)\n \ndecomp = SingularValueDecompositionImpl(A)\n \nprint decomp.getSingularValues()\n\nEnd code\n\nprints\narray('d', [11.218599757513008, 0.3781791648535976, nan])\nThe last singular value should be something very close to 0 since the matrix\nis rank deficient.  When i use the result from getSolver() to solve a system, i end \nup with a bunch of NaNs in the solution.  I assumed i would get back a least squares solution.\n\nDoes this SVD implementation require that the matrix be full rank?  If so, then i would expect\nan exception to be thrown from the constructor or one of the methods.\n\n\n",
    "desc_source": "jira"
  },
  "Math_77": {
    "description": "getLInfNorm() uses wrong formula in both ArrayRealVector and OpenMapRealVector (in different ways)\nthe L_infinity norm of a finite dimensional vector is just the max of the absolute value of its entries.\n\nThe current implementation in ArrayRealVector has a typo:\n\n{code}\n    public double getLInfNorm() {\n        double max = 0;\n        for (double a : data) {\n            max += Math.max(max, Math.abs(a));\n        }\n        return max;\n    }\n{code}\n\nthe += should just be an =.\n\nThere is sadly a unit test assuring us that this is the correct behavior (effectively a regression-only test, not a test for correctness).\n\nWorse, the implementation in OpenMapRealVector is not even positive semi-definite:\n\n{code}   \n    public double getLInfNorm() {\n        double max = 0;\n        Iterator iter = entries.iterator();\n        while (iter.hasNext()) {\n            iter.advance();\n            max += iter.value();\n        }\n        return max;\n    }\n{code}\n\nI would suggest that this method be moved up to the AbstractRealVector superclass and implemented using the sparseIterator():\n\n{code}\n  public double getLInfNorm() {\n    double norm = 0;\n    Iterator<Entry> it = sparseIterator();\n    Entry e;\n    while(it.hasNext() && (e = it.next()) != null) {\n      norm = Math.max(norm, Math.abs(e.getValue()));\n    }\n    return norm;\n  }\n{code}\n\nUnit tests with negative valued vectors would be helpful to check for this kind of thing in the future.",
    "desc_source": "jira"
  },
  "Math_78": {
    "description": "during ODE integration, the last event in a pair of very close event may not be detected\nWhen an events follows a previous one very closely, it may be ignored. The occurrence of the bug depends on the side of the bracketing interval that was selected. For example consider a switching function that is increasing around first event around t = 90, reaches its maximum and is decreasing around the second event around t = 135. If an integration step spans from 67.5 and 112.5, the switching function values at start and end of step will  have opposite signs, so the first event will be detected. The solver will find the event really occurs at 90.0 and will therefore truncate the step at 90.0. The next step will start from where the first step ends, i.e. it will start at 90.0. Let's say this step spans from 90.0 to 153.0. The switching function switches once again in this step.\n\nIf the solver for the first event converged to a value slightly before 90.0 (say 89.9999999), then the switch will not be detected because g(89.9999999) and g(153.0) are both negative.\n\nThis bug was introduced as of r781157 (2009-06-02) when special handling of events very close to step start was added.",
    "desc_source": "jira"
  },
  "Math_79": {
    "description": "NPE in  KMeansPlusPlusClusterer unittest\nWhen running this unittest, I am facing this NPE:\njava.lang.NullPointerException\n\tat org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.assignPointsToClusters(KMeansPlusPlusClusterer.java:91)\n\nThis is the unittest:\n\n\npackage org.fao.fisheries.chronicles.calcuation.cluster;\n\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertTrue;\n\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.Random;\n\nimport org.apache.commons.math.stat.clustering.Cluster;\nimport org.apache.commons.math.stat.clustering.EuclideanIntegerPoint;\nimport org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer;\nimport org.fao.fisheries.chronicles.input.CsvImportProcess;\nimport org.fao.fisheries.chronicles.input.Top200Csv;\nimport org.junit.Test;\n\npublic class ClusterAnalysisTest {\n\n\n\t@Test\n\tpublic void testPerformClusterAnalysis2() {\n\t\tKMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>(\n\t\t\t\tnew Random(1746432956321l));\n\t\tEuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] {\n\t\t\t\tnew EuclideanIntegerPoint(new int[] { 1959, 325100 }),\n\t\t\t\tnew EuclideanIntegerPoint(new int[] { 1960, 373200 }), };\n\t\tList<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1);\n\t\tassertEquals(1, clusters.size());\n\n\t}\n\n}\n",
    "desc_source": "jira"
  },
  "Math_80": {
    "description": "wrong result in eigen decomposition\nSome results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0\n{code}\n    public void testMathpbx02() {\n\n        double[] mainTridiagonal = {\n        \t  7484.860960227216, 18405.28129035345, 13855.225609560746,\n        \t 10016.708722343366, 559.8117399576674, 6750.190788301587, \n        \t    71.21428769782159\n        };\n        double[] secondaryTridiagonal = {\n        \t -4175.088570476366,1975.7955858241994,5193.178422374075, \n        \t  1995.286659169179,75.34535882933804,-234.0808002076056\n        };\n\n        // the reference values have been computed using routine DSTEMR\n        // from the fortran library LAPACK version 3.2.1\n        double[] refEigenValues = {\n        \t\t20654.744890306974412,16828.208208485466457,\n        \t\t6893.155912634994820,6757.083016675340332,\n        \t\t5887.799885688558788,64.309089923240379,\n        \t\t57.992628792736340\n        };\n        RealVector[] refEigenVectors = {\n        \t\tnew ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),\n        \t\tnew ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),\n        \t\tnew ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),\n        \t\tnew ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),\n        \t\tnew ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),\n        \t\tnew ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),\n        \t\tnew ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})\n        };\n\n        // the following line triggers the exception\n        EigenDecomposition decomposition =\n            new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);\n\n        double[] eigenValues = decomposition.getRealEigenvalues();\n        for (int i = 0; i < refEigenValues.length; ++i) {\n            assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);\n            if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {\n                assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);\n            } else {\n                assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);\n            }\n        }\n\n    }\n{code}",
    "desc_source": "jira"
  },
  "Math_81": {
    "description": "ArrayIndexOutOfBoundException in EigenDecompositionImpl\nThe following test triggers an ArrayIndexOutOfBoundException:\n\n{code:java}\n    public void testMath308() {\n\n        double[] mainTridiagonal = {\n            22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437\n        };\n        double[] secondaryTridiagonal = {\n            13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225\n        };\n\n        // the reference values have been computed using routine DSTEMR\n        // from the fortran library LAPACK version 3.2.1\n        double[] refEigenValues = {\n            14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002\n        };\n        RealVector[] refEigenVectors = {\n            new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),\n            new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),\n            new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),\n            new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),\n            new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })\n        };\n\n        // the following line triggers the exception\n        EigenDecomposition decomposition =\n            new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);\n\n        double[] eigenValues = decomposition.getRealEigenvalues();\n        for (int i = 0; i < refEigenValues.length; ++i) {\n            assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);\n            if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {\n                assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);\n            } else {\n                assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);\n            }\n        }\n\n    }\n{code}\n\nRunning the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:\n\n{noformat}\njava.lang.ArrayIndexOutOfBoundsException: -1\n\tat org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545)\n\tat org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072)\n\tat org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894)\n\tat org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658)\n\tat org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246)\n\tat org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205)\n\tat org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)\n{noformat}\n\nI'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.",
    "desc_source": "jira"
  },
  "Math_82": {
    "description": "SimplexSolver not working as expected 2\nSimplexSolver didn't find the optimal solution.\n\nProgram for Lpsolve:\n=====================\n/* Objective function */\nmax: 7 a 3 b;\n\n/* Constraints */\nR1: +3 a -5 c <= 0;\nR2: +2 a -5 d <= 0;\nR3: +2 b -5 c <= 0;\nR4: +3 b -5 d <= 0;\nR5: +3 a +2 b <= 5;\nR6: +2 a +3 b <= 5;\n\n/* Variable bounds */\na <= 1;\nb <= 1;\n=====================\nResults(correct): a = 1, b = 1, value = 10\n\n\nProgram for SimplexSolve:\n=====================\nLinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[]{7, 3, 0, 0}, 0);\nCollection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>();\npodmienky.add(new LinearConstraint(new double[]{1, 0, 0, 0}, Relationship.LEQ, 1));\npodmienky.add(new LinearConstraint(new double[]{0, 1, 0, 0}, Relationship.LEQ, 1));\npodmienky.add(new LinearConstraint(new double[]{3, 0, -5, 0}, Relationship.LEQ, 0));\npodmienky.add(new LinearConstraint(new double[]{2, 0, 0, -5}, Relationship.LEQ, 0));\npodmienky.add(new LinearConstraint(new double[]{0, 2, -5, 0}, Relationship.LEQ, 0));\npodmienky.add(new LinearConstraint(new double[]{0, 3, 0, -5}, Relationship.LEQ, 0));\npodmienky.add(new LinearConstraint(new double[]{3, 2, 0, 0}, Relationship.LEQ, 5));\npodmienky.add(new LinearConstraint(new double[]{2, 3, 0, 0}, Relationship.LEQ, 5));\nSimplexSolver solver = new SimplexSolver();\nRealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true);\n=====================\nResults(incorrect): a = 1, b = 0.5, value = 8.5\n\nP.S. I used the latest software from the repository (including MATH-286 fix).",
    "desc_source": "jira"
  },
  "Math_83": {
    "description": "SimplexSolver not working as expected?\nI guess (but I could be wrong) that SimplexSolver does not always return the optimal solution, nor satisfies all the constraints...\n\nConsider this LP:\n\nmax: 0.8 x0 + 0.2 x1 + 0.7 x2 + 0.3 x3 + 0.6 x4 + 0.4 x5;\nr1: x0 + x2 + x4 = 23.0;\nr2: x1 + x3 + x5 = 23.0;\nr3: x0 >= 10.0;\nr4: x2 >= 8.0;\nr5: x4 >= 5.0;\n\nLPSolve returns 25.8, with x0 = 10.0, x1 = 0.0, x2 = 8.0, x3 = 0.0, x4 = 5.0, x5 = 23.0;\n\nThe same LP expressed in Apache commons math is:\n\nLinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3, 0.6, 0.4 }, 0 );\nCollection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();\nconstraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0, 1, 0 }, Relationship.EQ, 23.0));\nconstraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1, 0, 1 }, Relationship.EQ, 23.0));\nconstraints.add(new LinearConstraint(new double[] { 1, 0, 0, 0, 0, 0 }, Relationship.GEQ, 10.0));\nconstraints.add(new LinearConstraint(new double[] { 0, 0, 1, 0, 0, 0 }, Relationship.GEQ, 8.0));\nconstraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 1, 0 }, Relationship.GEQ, 5.0));\n\nRealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);\n\nthat returns 22.20, with x0 = 15.0, x1 = 23.0, x2 = 8.0, x3 = 0.0, x4 = 0.0, x5 = 0.0;\n\nIs it possible SimplexSolver is buggy that way? The returned value is 22.20 instead of 25.8, and the last constraint (x4 >= 5.0) is not satisfied...\n\nAm I using the interface wrongly?",
    "desc_source": "jira"
  },
  "Math_84": {
    "description": "MultiDirectional optimzation loops forver if started at the correct solution\nMultiDirectional.iterateSimplex loops forever if the starting point is the correct solution.\n\nsee the attached test case (testMultiDirectionalCorrectStart) as an example.",
    "desc_source": "jira"
  },
  "Math_85": {
    "description": "bug in inverseCumulativeProbability() for Normal Distribution\n\n * @version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $\n */\npublic class NormalDistributionImpl extends AbstractContinuousDistribution \n\n\n * @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $\n */\npublic abstract class AbstractContinuousDistribution\n\n\nThis code:\n\n        \tDistributionFactory factory = app.getDistributionFactory();\n        \tNormalDistribution normal = factory.createNormalDistribution(0,1);\n        \tdouble result = normal.inverseCumulativeProbability(0.9772498680518209);\n\ngives the exception below. It should return (approx) 2.0000...\n\nnormal.inverseCumulativeProbability(0.977249868051820); works fine\n\nThese also give errors:\n0.9986501019683698 (should return 3.0000...)\n0.9999683287581673 (should return 4.0000...)\n\norg.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0\n\tat org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103)\n\tat org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)\n\n\n",
    "desc_source": "jira"
  },
  "Math_86": {
    "description": "testing for symmetric positive definite matrix in CholeskyDecomposition\nI used this matrix:\n\n        double[][] cv = {\n            {0.40434286, 0.09376327, 0.30328980, 0.04909388},\n            {0.09376327, 0.10400408, 0.07137959, 0.04762857},\n            {0.30328980, 0.07137959, 0.30458776, 0.04882449},\n            {0.04909388, 0.04762857, 0.04882449, 0.07543265}\n        };\n\nAnd it works fine, because it is symmetric positive definite\n\nI tried this matrix:\n\n        double[][] cv = {\n            {0.40434286, -0.09376327, 0.30328980, 0.04909388},\n            {-0.09376327, 0.10400408, 0.07137959, 0.04762857},\n            {0.30328980, 0.07137959, 0.30458776, 0.04882449},\n            {0.04909388, 0.04762857, 0.04882449, 0.07543265}\n        };\n\nAnd it should throw an exception but it does not.  I tested the matrix in R and R's cholesky decomposition method returns that the matrix is not symmetric positive definite.\n\nObviously your code is not catching this appropriately.\n\nBy the way (in my opinion) the use of exceptions to check these conditions is not the best design or use for exceptions.  If you are going to force the use to try and catch these exceptions at least provide methods  to test the conditions prior to the possibility of the exception.  \n\n",
    "desc_source": "jira"
  },
  "Math_87": {
    "description": "Basic variable is not found correctly in simplex tableau\nThe last patch to SimplexTableau caused an automated test suite I'm running at work to go down a new code path and uncover what is hopefully the last bug remaining in the Simplex code.\nSimplexTableau was assuming an entry in the tableau had to be nonzero to indicate a basic variable, which is incorrect - the entry should have a value equal to 1.",
    "desc_source": "jira"
  },
  "Math_88": {
    "description": "Simplex Solver arrives at incorrect solution\nI have reduced the problem reported to me down to a minimal test case which I will attach.",
    "desc_source": "jira"
  },
  "Math_89": {
    "description": "Bugs in Frequency API\nI think the existing Frequency API has some bugs in it.\n\nThe addValue(Object v) method allows one to add a plain Object, but one cannot add anything further to the instance, as the second add fails with IllegalArgumentException.\nIn fact, the problem is with the first call to addValue(Object) which should not allow a plain Object to be added - it should only allow Comparable objects.\nThis could be fixed by checking that the object is Comparable.\n\nSimilar considerations apply to the getCumFreq(Object) and getCumPct(Object) methods - they will only work with objects that implement Comparable.\n\nThe getCount(Object) and getPct(Object) methods don't fail when given a non-Comparable object (because the class cast exception is caught), however they just return 0 as if the object was not present:\n\n{code}\n        final Object OBJ = new Object();\n        f.addValue(OBJ); // This ought to fail, but doesn't, causing the unexpected behaviour below\n        System.out.println(f.getCount(OBJ)); // 0\n        System.out.println(f.getPct(OBJ)); // 0.0\n{code}\n\nRather than adding extra checks for Comparable, it seems to me that the API would be much improved by using Comparable instead of Object.\nAlso, it should make it easier to implement generics.\n\nHowever, this would cause compilation failures for some programs that pass Object rather than Comparable to the class.\nThese would need recoding, but I think they would continue to run OK against the new API.\n\nIt would also affect the run-time behaviour slightly, as the first attempt to add a non-Comparable object would fail, rather than the second add of a possibly valid object.\nBut is that a viable program? It can only add one object, and any attempt to get statistics will either return 0 or an Exception, and applying the instanceof fix would also cause it to fail.",
    "desc_source": "jira"
  },
  "Math_90": {
    "description": "Bugs in Frequency API\nI think the existing Frequency API has some bugs in it.\n\nThe addValue(Object v) method allows one to add a plain Object, but one cannot add anything further to the instance, as the second add fails with IllegalArgumentException.\nIn fact, the problem is with the first call to addValue(Object) which should not allow a plain Object to be added - it should only allow Comparable objects.\nThis could be fixed by checking that the object is Comparable.\n\nSimilar considerations apply to the getCumFreq(Object) and getCumPct(Object) methods - they will only work with objects that implement Comparable.\n\nThe getCount(Object) and getPct(Object) methods don't fail when given a non-Comparable object (because the class cast exception is caught), however they just return 0 as if the object was not present:\n\n{code}\n        final Object OBJ = new Object();\n        f.addValue(OBJ); // This ought to fail, but doesn't, causing the unexpected behaviour below\n        System.out.println(f.getCount(OBJ)); // 0\n        System.out.println(f.getPct(OBJ)); // 0.0\n{code}\n\nRather than adding extra checks for Comparable, it seems to me that the API would be much improved by using Comparable instead of Object.\nAlso, it should make it easier to implement generics.\n\nHowever, this would cause compilation failures for some programs that pass Object rather than Comparable to the class.\nThese would need recoding, but I think they would continue to run OK against the new API.\n\nIt would also affect the run-time behaviour slightly, as the first attempt to add a non-Comparable object would fail, rather than the second add of a possibly valid object.\nBut is that a viable program? It can only add one object, and any attempt to get statistics will either return 0 or an Exception, and applying the instanceof fix would also cause it to fail.",
    "desc_source": "jira"
  },
  "Math_91": {
    "description": "Fraction.comparTo returns 0 for some differente fractions\nIf two different fractions evaluate to the same double due to limited precision,\nthe compareTo methode returns 0 as if they were identical.\n\n{code}\n// value is roughly PI - 3.07e-18\nFraction pi1 = new Fraction(1068966896, 340262731);\n\n// value is roughly PI + 1.936e-17\nFraction pi2 = new Fraction( 411557987, 131002976);\n\nSystem.out.println(pi1.doubleValue() - pi2.doubleValue()); // exactly 0.0 due to limited IEEE754 precision\nSystem.out.println(pi1.compareTo(pi2)); // display 0 instead of a negative value\n{code}",
    "desc_source": "jira"
  },
  "Math_92": {
    "description": "MathUtils.binomialCoefficient(n,k) fails for large results\nProbably due to rounding errors, MathUtils.binomialCoefficient(n,k) fails for results near Long.MAX_VALUE.\n\nThe existence of failures can be demonstrated by testing the recursive property:\n\n{noformat}\n         assertEquals(MathUtils.binomialCoefficient(65,32) + MathUtils.binomialCoefficient(65,33),\n                 MathUtils.binomialCoefficient(66,33));\n{noformat}\n\nOr by directly using the (externally calculated and hopefully correct) expected value:\n\n{noformat}\n         assertEquals(7219428434016265740L, MathUtils.binomialCoefficient(66,33));\n{noformat}\n\nI suggest a nonrecursive test implementation along the lines of\n\n{code:title=MathUtilsTest.java|borderStyle=solid}\n    /**\n     * Exact implementation using BigInteger and the explicit formula\n     * (n, k) == ((k-1)*...*n) / (1*...*(n-k))\n     */\n\tpublic static long binomialCoefficient(int n, int k) {\n\t\tif (k == 0 || k == n)\n\t\t\treturn 1;\n\t\tBigInteger result = BigInteger.ONE;\n\t\tfor (int i = k + 1; i <= n; i++) {\n\t\t\tresult = result.multiply(BigInteger.valueOf(i));\n\t\t}\n\t\tfor (int i = 1; i <= n - k; i++) {\n\t\t\tresult = result.divide(BigInteger.valueOf(i));\n\t\t}\n\t\tif (result.compareTo(BigInteger.valueOf(Long.MAX_VALUE)) > 0) {\n\t\t\tthrow new ArithmeticException(\n                                \"Binomial coefficient overflow: \" + n + \", \" + k);\n\t\t}\n\t\treturn result.longValue();\n\t}\n{code} \n\nWhich would allow you to test the expected values directly:\n\n{noformat}\n         assertEquals(binomialCoefficient(66,33), MathUtils.binomialCoefficient(66,33));\n{noformat}\n",
    "desc_source": "jira"
  },
  "Math_93": {
    "description": "MathUtils.factorial(n) fails for n >= 17\nThe result of MathUtils.factorial( n ) for n = 17, 18, 19 is wrong, probably because of rounding errors in the double calculations.\n\nReplace the first line of MathUtilsTest.testFactorial() by\n\n        for (int i = 1; i <= 20; i++) {\n\nto check all valid arguments for the long result and see the failure.\n\nI suggest implementing a simple loop to multiply the long result - or even using a precomputed long[] - instead of adding logarithms.",
    "desc_source": "jira"
  },
  "Math_94": {
    "description": "MathUtils.gcd(u, v) fails when u and v both contain a high power of 2\nThe test at the beginning of MathUtils.gcd(u, v) for arguments equal to zero fails when u and v contain high enough powers of 2 so that their product overflows to zero.\n\n        assertEquals(3 * (1<<15), MathUtils.gcd(3 * (1<<20), 9 * (1<<15)));\n\nFix: Replace the test at the start of MathUtils.gcd()\n\n        if (u * v == 0) {\n\nby\n\n        if (u == 0 || v == 0) {\n",
    "desc_source": "jira"
  },
  "Math_95": {
    "description": "denominatorDegreeOfFreedom in FDistribution leads to IllegalArgumentsException in UnivariateRealSolverUtils.bracket\nWe are using the FDistributionImpl from the commons.math project to do\nsome statistical calculations, namely receiving the upper and lower\nboundaries of a confidence interval. Everything is working fine and the\nresults are matching our reference calculations.\n\nHowever, the FDistribution behaves strange if a\ndenominatorDegreeOfFreedom of 2 is used, with an alpha-value of 0.95.\nThis results in an IllegalArgumentsException, stating:\n        \nInvalid endpoint parameters:  lowerBound=0.0 initial=Infinity\nupperBound=1.7976931348623157E308\n        \ncoming from\norg.apache.commons.math.analysis.UnivariateRealSolverUtils.bracket\n        \nThe problem is the 'initial' parameter to that function, wich is\nPOSITIVE_INFINITY and therefore not within the boundaries. I already\npinned down the problem to the FDistributions getInitialDomain()-method,\nwich goes like:\n\n        return getDenominatorDegreesOfFreedom() /\n                    (getDenominatorDegreesOfFreedom() - 2.0);\n        \nObviously, in case of denominatorDegreesOfFreedom == 2, this must lead\nto a division-by-zero, resulting in POSTIVE_INFINITY. The result of this\noperation is then directly passed into the\nUnivariateRealSolverUtils.bracket() - method as second argument.",
    "desc_source": "jira"
  },
  "Math_96": {
    "description": "Result of multiplying and equals for complex numbers is wrong\nHi.\n\nThe bug relates on complex numbers.\nThe methods \"multiply\" and \"equals\" of the class Complex are involved.\n\nmathematic background:  (0,i) * (-1,0i) = (0,-i).\n\nlittle java program + output that shows the bug:\n-----------------------------------------------------------------------\n{code}\nimport org.apache.commons.math.complex.*;\npublic class TestProg {\n        public static void main(String[] args) {\n\n                ComplexFormat f = new ComplexFormat();\n                Complex c1 = new Complex(0,1);\n                Complex c2 = new Complex(-1,0);\n\n                Complex res = c1.multiply(c2);\n                Complex comp = new Complex(0,-1);\n\n                System.out.println(\"res:  \"+f.format(res));\n                System.out.println(\"comp: \"+f.format(comp));\n\n                System.out.println(\"res=comp: \"+res.equals(comp));\n        }\n}\n{code}\n-----------------------------------------------------------------------\n\nres:  -0 - 1i\ncomp: 0 - 1i\nres=comp: false\n\n-----------------------------------------------------------------------\n\nI think the \"equals\" should return \"true\".\nThe problem could either be the \"multiply\" method that gives (-0,-1i) instead of (0,-1i),\nor if you think thats right, the equals method has to be modified.\n\nGood Luck\nDieter",
    "desc_source": "jira"
  },
  "Math_97": {
    "description": "BrentSolver throws IllegalArgumentException \nI am getting this exception:\n\njava.lang.IllegalArgumentException: Function values at endpoints do not have different signs.  Endpoints: [-100000.0,1.7976931348623157E308]  Values: [0.0,-101945.04630982173]\nat org.apache.commons.math.analysis.BrentSolver.solve(BrentSolver.java:99)\nat org.apache.commons.math.analysis.BrentSolver.solve(BrentSolver.java:62)\n\nThe exception should not be thrown with values  [0.0,-101945.04630982173] because 0.0 is positive.\nAccording to Brent Worden, the algorithm should stop and return 0 as the root instead of throwing an exception.\n\nThe problem comes from this method:\n    public double solve(double min, double max) throws MaxIterationsExceededException, \n        FunctionEvaluationException {\n        \n        clearResult();\n        verifyInterval(min, max);\n        \n        double yMin = f.value(min);\n        double yMax = f.value(max);\n        \n        // Verify bracketing\n        if (yMin * yMax >= 0) {\n            throw new IllegalArgumentException\n            (\"Function values at endpoints do not have different signs.\" +\n                    \"  Endpoints: [\" + min + \",\" + max + \"]\" + \n                    \"  Values: [\" + yMin + \",\" + yMax + \"]\");       \n        }\n\n        // solve using only the first endpoint as initial guess\n        return solve(min, yMin, max, yMax, min, yMin);\n\n    }\n\nOne way to fix it would be to add this code after the assignment of yMin and yMax:\n        if (yMin ==0 || yMax == 0) {\n        \treturn 0;\n       \t}\n",
    "desc_source": "jira"
  },
  "Math_98": {
    "description": "RealMatrixImpl#operate gets result vector dimensions wrong\n{{org.apache.commons.math.linear.RealMatrixImpl#operate}} tries to create a result vector that always has the same length as the input vector. This can result in runtime exceptions if the matrix is non-square and it always yields incorrect results if the matrix is non-square. The correct behaviour would of course be to create a vector with the same length as the row dimension of the matrix.\n\nThus line 640 in RealMatrixImpl.java should read\n  {{double[] out = new double[nRows];}}\ninstead of\n  {{double[] out = new double[v.length];}}\n",
    "desc_source": "jira"
  },
  "Math_99": {
    "description": "MathUtils.gcd(Integer.MIN_VALUE, 0) should throw an Exception instead of returning Integer.MIN_VALUE\nThe gcd method should throw an Exception for gcd(Integer.MIN_VALUE, 0), like for gcd(Integer.MIN_VALUE, Integer.MIN_VALUE). The method should only return nonnegative results.",
    "desc_source": "jira"
  },
  "Math_100": {
    "description": "AbstractEstimator: getCovariances() and guessParametersErrors() crash when having bound parameters\nthe two methods getCovariances() and guessParametersErrors() from org.apache.commons.math.estimation.AbstractEstimator crash with ArrayOutOfBounds exception when some of the parameters are bound. The reason is that the Jacobian is calculated only for the unbound parameters. in the code you loop through all parameters.\n\nline #166: final int cols = problem.*getAllParameters()*.length;\nshould be replaced by:  final int cols = problem.*getUnboundParameters()*.length;\n(similar changes could be done in guessParametersErrors())\n\nthe dissadvantage of the above bug fix is that what is returned to the user is an array with smaller size than the number of all parameters. Alternatively, you can have some logic in the code which writes zeros for the elements of the covariance matrix corresponding to the bound parameters",
    "desc_source": "jira"
  },
  "Math_101": {
    "description": "java.lang.StringIndexOutOfBoundsException in ComplexFormat.parse(String source, ParsePosition pos)\nThe parse(String source, ParsePosition pos) method in the ComplexFormat class does not check whether the imaginary character is set or not which produces StringIndexOutOfBoundsException in the substring method :\n\n(line 375 of ComplexFormat)\n...\n        // parse imaginary character\n        int n = getImaginaryCharacter().length();\n        \n        startIndex = pos.getIndex();\n        int endIndex = startIndex + n;\n        if (source.substring(startIndex, endIndex).compareTo(\n            getImaginaryCharacter()) != 0) {\n...\nI encoutered this exception typing in a JTextFied with ComplexFormat set to look up an AbstractFormatter.\nIf only the user types the imaginary part of the complex number first, he gets this exception.\n\nSolution: Before setting to n length of the imaginary character, check if the source contains it. My proposal:\n...\n        int n = 0;\n        if (source.contains(getImaginaryCharacter()))\n        n = getImaginaryCharacter().length();\n...\t\t \n\nF.S.",
    "desc_source": "jira"
  },
  "Math_102": {
    "description": "chiSquare(double[] expected, long[] observed) is returning incorrect test statistic\nChiSquareTestImpl is returning incorrect chi-squared value. An implicit assumption of public double chiSquare(double[] expected, long[] observed) is that the sum of expected and observed are equal. That is, in the code:\nfor (int i = 0; i < observed.length; i++) {\n            dev = ((double) observed[i] - expected[i]);\n            sumSq += dev * dev / expected[i];\n        }\nthis calculation is only correct if sum(observed)==sum(expected). When they are not equal then one must rescale the expected value by sum(observed) / sum(expected) so that they are.\nIronically, it is an example in the unit test ChiSquareTestTest that highlights the error:\n\nlong[] observed1 = { 500, 623, 72, 70, 31 };\n        double[] expected1 = { 485, 541, 82, 61, 37 };\n        assertEquals( \"chi-square test statistic\", 16.4131070362, testStatistic.chiSquare(expected1, observed1), 1E-10);\n        assertEquals(\"chi-square p-value\", 0.002512096, testStatistic.chiSquareTest(expected1, observed1), 1E-9);\n\n16.413 is not correct because the expected values do not make sense, they should be: 521.19403 581.37313  88.11940  65.55224  39.76119 so that the sum of expected equals 1296 which is the sum of observed.\n\nHere is some R code (r-project.org) which proves it:\n> o1\n[1] 500 623  72  70  31\n> e1\n[1] 485 541  82  61  37\n> chisq.test(o1,p=e1,rescale.p=TRUE)\n\n        Chi-squared test for given probabilities\n\ndata:  o1 \nX-squared = 9.0233, df = 4, p-value = 0.06052\n\n> chisq.test(o1,p=e1,rescale.p=TRUE)$observed\n[1] 500 623  72  70  31\n> chisq.test(o1,p=e1,rescale.p=TRUE)$expected\n[1] 521.19403 581.37313  88.11940  65.55224  39.76119\n\n\n\n\n\n ",
    "desc_source": "jira"
  },
  "Math_103": {
    "description": "ConvergenceException in normal CDF\nNormalDistributionImpl::cumulativeProbability(double x) throws ConvergenceException\nif x deviates too much from the mean. For example, when x=+/-100, mean=0, sd=1.\nOf course the value of the CDF is hard to evaluate in these cases,\nbut effectively it should be either zero or one.",
    "desc_source": "jira"
  },
  "Math_104": {
    "description": "Special functions not very accurate\nThe Gamma and Beta functions return values in double precision but the default epsilon is set to 10e-9. I think that the default should be set to the highest possible accuracy, as this is what I'd expect to be returned by a double precision routine. Note that the erf function already uses a call to Gamma.regularizedGammaP with an epsilon of 1.0e-15.",
    "desc_source": "jira"
  },
  "Math_105": {
    "description": "[math]  SimpleRegression getSumSquaredErrors\ngetSumSquaredErrors returns -ve value. See test below:\n\npublic void testSimpleRegression() {\n\t\tdouble[] y = {  8915.102, 8919.302, 8923.502};\n\t\tdouble[] x = { 1.107178495, 1.107264895, 1.107351295};\n\t\tdouble[] x2 = { 1.107178495E2, 1.107264895E2, 1.107351295E2};\n\t\tSimpleRegression reg = new SimpleRegression();\n\t\tfor (int i = 0; i < x.length; i++) {\n\t\t\treg.addData(x[i],y[i]);\n\t\t}\n\t\tassertTrue(reg.getSumSquaredErrors() >= 0.0); // OK\n\t\treg.clear();\n\t\tfor (int i = 0; i < x.length; i++) {\n\t\t\treg.addData(x2[i],y[i]);\n\t\t}\n\t\tassertTrue(reg.getSumSquaredErrors() >= 0.0); // FAIL\n\t\t\n\t}",
    "desc_source": "jira"
  },
  "Math_106": {
    "description": "[math] Function math.fraction.ProperFractionFormat.parse(String, ParsePosition) return illogical result\nHello,\n\nI find illogical returned result from function \"Fraction parse(String source, \nParsePostion pos)\" (in class ProperFractionFormat of the Fraction Package) of \nthe Commons Math library. Please see the following code segment for more \ndetails:\n\n\"\nProperFractionFormat properFormat = new ProperFractionFormat();\nresult = null;\nString source = \"1 -1 / 2\";\nParsePosition pos = new ParsePosition(0);\n\n//Test 1 : fail \npublic void testParseNegative(){\n \n   String source = \"-1 -2 / 3\";\n   ParsePosition pos = new ParsePosition(0);\n\n   Fraction actual = properFormat.parse(source, pos);\n   assertNull(actual);\n}\n\n// Test2: success\npublic void testParseNegative(){\n \n   String source = \"-1 -2 / 3\";\n   ParsePosition pos = new ParsePosition(0);\n\n   Fraction actual = properFormat.parse(source, pos);  // return Fraction 1/3\n   assertEquals(1, source.getNumerator());\n   assertEquals(3, source.getDenominator());\n}\n\n\"\n\nNote: Similarly, when I passed in the following inputs: \n  input 2: (source = \u00931 2 / -3\u0094, pos = 0)\n  input 3: ( source = \u0094 -1 -2 / 3\u0094, pos = 0)\n\nFunction \"Fraction parse(String, ParsePosition)\" returned Fraction 1/3 (means \nthe result Fraction had numerator = 1 and  denominator = 3)for all 3 inputs \nabove.\n \nI think the function does not handle parsing the numberator/ denominator \nproperly incase input string provide invalid numerator/denominator. \n\nThank you!",
    "desc_source": "jira"
  }
}